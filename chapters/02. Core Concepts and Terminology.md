---
title: "Core Concepts and Terminology"
permalink: /chapters/concepts/
---
# Core Concepts and Terminology

## Accidents, hazards, and losses

Safety engineering usually distinguishes **what can go wrong**, **how it happens**, and **what it costs**.

* **Loss / harm (consequence):** the negative outcome you care about preventing (e.g., injury, death, property damage, environmental damage, mission loss, major financial loss). Many safety standards and handbooks explicitly include these consequence types when discussing severity and mishap outcomes. ([cto.mil][1])
* **Accident / mishap:** the realized, undesired event that produces losses (e.g., “robot strikes a person,” “UAV collides with vehicle,” “infusion pump delivers overdose”). NASA system safety literature commonly uses “mishap” for an unplanned event leading to loss. ([NASA Technical Reports Server][2])
* **Hazard:** a system condition, set of conditions, or potential causal factor that can contribute to an accident/mishap scenario. In system safety practice (e.g., DoD system safety), hazards are framed as conditions that could lead to mishaps and losses. ([cto.mil][1])

A useful mental model:

> **Hazard** (potential cause / unsafe condition) → **Accident/Mishap** (event) → **Loss** (harm/damage/mission impact)

This avoids a common developer mistake: treating “hazard” as “bad outcome.” The bad outcome is the **loss**; the hazard is what can lead you there.

## Risk basics

### Severity and likelihood (and limits of simple matrices)

Most industries define **risk** as combining:

* **Severity**: how bad the credible outcome is.
* **Likelihood**: how often the outcome (or the hazardous event leading to harm) might occur.

A common operationalization is a **risk matrix** (severity category × probability category), used in e.g. MIL-STD system safety processes to assign risk levels for hazards. ([cto.mil][1])

**Why matrices are useful (and why they mislead):**

* Useful for **triage** (what to look at first) and **communication**.
* But they can hide important structure:

  * Two hazards can end up with the same “cell” while being qualitatively different (rare-catastrophic vs frequent-moderate).
  * Category boundaries are subjective (what counts as “remote”?).
  * They can encourage “score chasing” rather than engineering controls.

Nancy Leveson (MIT) has a well-known critique of standard risk matrices and how they can produce inconsistent/unsafe decisions if treated as precise instruments rather than coarse aids. ([sunnyday.mit.edu][3])

Practical takeaway: **use matrices as a sorting heuristic**, not as proof that something is “safe.”

### Exposure, controllability, detectability (when used)

Some domains refine “likelihood” into scenario-based factors.

* **Exposure (E):** how often the system and environment are in a situation where the hazard could matter. In automotive functional safety, ASIL determination considers **severity, probability of exposure, and controllability**. ( [4])
* **Controllability (C):** how likely a human (or external actor) can avoid the harm once the hazardous situation occurs. This is part of ISO 26262 hazard analysis and risk assessment logic (HARA). ( [4])
* **Detectability (D):** commonly used in **FMEA-style prioritization** as “ability to detect the failure or its effects before it causes harm.” Traditional FMEA risk priority approaches use Severity–Occurrence–Detection (S×O×D) as an input to prioritization (often called RPN). ([ITEH Standards][5])

When to use what:

* **Severity × Probability**: early system safety triage, high-level hazard logs, when you need simple governance.
* **Severity × Exposure × Controllability**: when the *scenario* and human/operational context drives risk classification (e.g., road vehicles; often adapted in robotics and similar systems).
* **Severity × Occurrence × Detectability (FMEA)**: when analyzing failure modes and existing controls, especially in design/process quality + reliability workflows. ([ITEH Standards][5])

## Safety requirements

### From hazards to constraints (“must not …”)

A safety requirement is often best written as a **constraint on system behavior** derived from a hazard:

* Hazard: “Actuator may move while a person is in the safeguarded area.”
* Safety constraint: “The system **must not** command motion when the safety zone is violated.”

This “must not …” framing keeps you honest:

* It forces you to state the **unsafe control action** (what must be prevented).
* It pushes you to specify *when* it is unsafe (context, mode, timing, states).

In practice you’ll usually produce a chain:

1. **Hazard statement** (unsafe condition / causal factor)
2. **Accident / loss scenario** (how it becomes harm)
3. **Safety constraint** (“must not…”, “shall…”, “only if…”)
4. **Derived technical requirements** (signals, timing, diagnostics, interfaces, testable criteria)

### Safety functions vs safety mechanisms

These terms get mixed up; keeping them distinct helps with architecture and verification.

* **Safety function:** *what safety-related behavior must be achieved* to reduce risk (e.g., “bring system to a safe state if overspeed is detected”; “prevent hazardous motion when guard is open”). In functional safety standards, safety functions are the unit to which integrity/required risk reduction is often assigned (e.g., SIL targets describe required risk reduction for safety functions). ([assets.iec.ch][6])
* **Safety mechanism:** *how you implement that function* (watchdog, plausibility checks, redundancy, interlocks, heartbeat monitoring, safe torque off, CRC on messages, range checks, etc.).

Rule of thumb:

* A **function** should be specifiable without committing to an implementation.
* A **mechanism** is an implementation choice (and may have its own failure modes).

## Assumptions and operational design domain

### Environmental, user, and system assumptions

Every safety argument relies on assumptions. Make them explicit, because “unknown assumptions” become “unknown hazards.”

Typical assumption types:

* **Environmental assumptions:** operating temperature, lighting, EMI, weather, road/terrain, GNSS availability, expected obstacles, network conditions.
* **User assumptions (human factors):** training level, reaction time, adherence to procedures, correct use vs foreseeable misuse.
* **System assumptions:** sensor accuracy bounds, compute budget, timing guarantees, power integrity, calibration state, other subsystems behaving within contract.

In highly automated systems, these assumptions are often captured as an **Operational Design Domain (ODD)**: the set of operating conditions under which the system is designed to function (environmental, geographical, time-of-day constraints, roadway/traffic characteristics, etc.). SAE J3016’s definition is widely referenced; ISO 21448 (SOTIF) uses a closely related framing. ([asam.net][7])

Even if you’re not building a car: robotics/UAV systems still have an implicit “ODD” (wind limits, visibility, allowed airspace conditions, operator supervision assumptions).

### Handling assumption violations

Assumption violations are not edge cases; they are normal life.

Engineering patterns to handle them:

* **Detect** violation (health monitoring, sensor diagnostics, input plausibility, timing monitors, environment classification).
* **Degrade** gracefully (reduced speed/force, restricted modes, disable risky features outside ODD).
* **Transition to safe state** (stop motion, safe landing, power-down actuators, hold position, alert operator).
* **Communicate clearly** (operator warnings, logs for post-incident analysis, maintenance flags).

Document these explicitly as requirements, for example:

* “If environment classification indicates out-of-ODD condition X for >T seconds, the system shall enter mode Y within T2 seconds.”
* “If assumption A is violated and cannot be confirmed within N retries, the system shall inhibit function F and raise fault code Z.”

This is where software developers add disproportionate value: **monitoring, state management, and fault-response logic** often determine whether a real-world hazard becomes a real-world accident.

[1]: https://www.cto.mil/wp-content/uploads/2025/07/MIL-STD-882E-w_CHANGE-1.pdf?utm_source=chatgpt.com "department of defense standard practice system safety"
[2]: https://ntrs.nasa.gov/api/citations/20150015500/downloads/20150015500.pdf?utm_source=chatgpt.com "NASA System Safety Handbook"
[3]: https://sunnyday.mit.edu/Risk-Matrix.pdf?utm_source=chatgpt.com "Improving the Standard Risk Matrix: Part 11 - Nancy Leveson"
[4]: https://files.infocentre.io/files/docs_clients/126_2008096318_4226751_docu_nt_doga_ISO_26262-3.pdf?utm_source=chatgpt.com "INTERNATIONAL STANDARD ISO 26262-3 - Infocentre"
[5]: https://cdn.standards.iteh.ai/samples/21903/331059ef02794b548d049d58b18e456f/IEC-60812-2018.pdf?utm_source=chatgpt.com "IEC 60812:2018"
[6]: https://assets.iec.ch/public/acos/IEC%2061508%20%26%20Functional%20Safety-2022.pdf?2023040501=&utm_source=chatgpt.com "Overview of IEC 61508 & Functional Safety"
[7]: https://www.asam.net/index.php?eID=dumpFile&f=4303&t=f&token=3135965e578e5bb92a01725cd37823c3979da158&utm_source=chatgpt.com "The significance of the ODD for the SOTIF Process"
