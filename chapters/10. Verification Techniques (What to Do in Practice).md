---
title: "Verification Techniques"
permalink: /chapters/verification/
---
# Verification Techniques (What to Do in Practice)

Verification is how you build confidence that the software **implements the safety requirements correctly** and **does not introduce hazards through unintended behavior**. Safety-related standards consistently treat verification as a *mix* of **reviews**, **analyses (static + sometimes formal)**, and **testing**, often with added emphasis on **independence** for higher criticality (e.g., IV&V in NASA programs). ([  NASA][1])

A practical approach is to plan verification **around hazards and safety goals**:

* For each hazard/safety goal, identify **what could go wrong**, then pick verification activities that would *actually catch that class of failure*.
* Prefer techniques that find issues **early** (reviews, static analysis, small-model checking) and use tests to prove behavior **at interfaces and in scenarios**.

---

## Reviews and inspections

### What to review in safety-relevant code

Reviews are still one of the highest ROI techniques because many safety failures are **logic/state/interface mistakes**, not just crashes. Many safety processes explicitly require systematic reviews/analyses as part of verification. ([AdaCore Learning][2])

Use a **checklist** specialized for safety:

#### 1) Requirements ↔ implementation alignment

* Every safety requirement has a clear implementation point (or explicit rationale if not applicable).
* No “helpful” behavior that contradicts safety constraints (e.g., silently continuing after sensor invalid).
* No dead code / unreachable branches implementing safety logic (often indicates outdated assumptions).

#### 2) Hazard controls are explicit and robust

* Safety mechanisms (limits, interlocks, timeouts, plausibility checks) are:

  * **Centralized** (not duplicated inconsistently),
  * **Deterministic** in priority/ordering,
  * **Failing to safe state** on uncertainty (fail-closed vs fail-open is explicit).

#### 3) Inputs, units, and boundaries

* All external inputs are validated at trust boundaries (range, type, monotonicity, freshness).
* Units/frames are explicit (meters vs millimeters, radians vs degrees, coordinate frame assumptions).
* Bounds checks and saturation exist for numeric edge cases.

#### 4) Concurrency and timing

* Lock ordering is consistent; no blocking in real-time paths.
* Timeouts exist for external calls; watchdog/health monitoring behavior is defined.
* Shared state is protected (or made immutable / message-passing).

#### 5) Error handling policy

* No silent failures; errors are either propagated or handled with a documented safe fallback.
* Logging/telemetry is sufficient to diagnose safety-relevant faults without flooding.

#### 6) “Tooling and change” hygiene

* Safety-critical files have stricter review rules (2 reviewers, or reviewer independence).
* Changes include updated tests and updated hazard/assumption references.

**Tip:** Treat review findings as “defects against hazards”, not stylistic comments—force a clear statement like *“This could violate safety goal SG-3 under sensor dropout.”*

---

## Static analysis

Static analysis is most effective when it is **policy-driven** (what you forbid, what you require evidence for), and when results are kept near-zero “known warnings”.

### Common layers (from simplest to strongest)

#### 1) Linters and style rules (consistency that prevents mistakes)

* Enforce “no unused results”, “no implicit fallthrough”, “no shadowing”, “no magic numbers in safety logic”.
* Keep the rule set small, then tighten it over time.

#### 2) Type checking (reduce illegal states)

* Use strong typing to prevent unit confusion and invalid states.
* Type checking is especially valuable where runtime failures are expensive to reproduce.

#### 3) Sanitizers (dynamic analysis via instrumentation)

For C/C++:

* **AddressSanitizer** detects memory errors like out-of-bounds and use-after-free. ([clang.llvm.org][3])
* **UndefinedBehaviorSanitizer** detects classes of undefined behavior (often safety-relevant because UB can become nondeterministic). ([clang.llvm.org][4])
  (Also consider ThreadSanitizer for races in threaded code, when applicable.)

#### 4) MISRA-like rules (concept)

For safety-related embedded C/C++ code, a common pattern is to adopt:

* **Restricted language subsets** + rule checking (e.g., MISRA C) to avoid hazardous constructs and improve analyzability. ([الکترو ولت][5])
* A **documented deviation process** (some violations are inevitable; they must be justified and reviewed). ([misra.org.uk][6])

### Practical policy

* **Gate merges** on: new high-severity issues, new warnings in safety-critical modules, missing suppressions/rationales.
* Track “static analysis debt” explicitly; do not allow “warning creep”.
* If you rely on tools for safety evidence, be aware that some safety standards treat tool failures as a risk category and may require justification/qualification depending on use. ([61508.org][7])

---

## Formal methods

Formal methods are techniques that use **mathematical or logic-based specifications** to **prove or exhaustively check** properties of a system (e.g., “this bad state is unreachable”). In practice, most software teams don’t apply full formal verification to entire systems; instead, they use *lightweight* formal methods on the parts where subtle mistakes are common and consequences are high (mode logic, concurrency protocols, safety interlocks).

### What formal methods give you (and what they don’t)

**They are good at:**

* catching “impossible” corner cases (rare interleavings, forgotten transitions),
* proving **invariants** (“always true”) and **safety properties** (“never do X”),
* exploring large state spaces automatically (model checking).

**They are not a replacement for:**

* tests with real sensors/hardware timing,
* validation of human factors or operational assumptions,
* performance verification.

Think of them as: **a way to make reasoning about complex logic rigorous**, especially where testing would miss rare paths.

### Key concepts (minimal vocabulary)

* **State**: what the system “remembers” (mode, counters, timers, flags).
* **Transition**: how state changes on events (message arrives, timeout fires, fault detected).
* **Invariant**: a condition that must always hold (e.g., “ARMED implies brakes released”).
* **Safety property** (“nothing bad happens”): e.g., “never command thrust > limit”.
* **Liveness property** (“something good eventually happens”): e.g., “if fault persists, system eventually enters SAFE within T”.

### Common lightweight techniques

#### 1) Contracts (preconditions / postconditions / invariants)

“Design by contract” adds explicit statements about what a function/component **requires** and **guarantees**:

* **Preconditions**: what must be true for correct use (inputs valid, state allows call).
* **Postconditions**: what will be true after success (outputs within bounds, state updated).
* **Invariants**: what must remain true for the component/system at all times.

This improves safety because it turns hidden assumptions into checkable statements and reduces “undefined behavior by convention”.

How teams apply it lightly:

* runtime `assert`/checks in debug builds,
* “validate()” functions on critical data structures,
* tests that specifically target contract boundaries,
* documentation that is precise enough to be mechanically checked later.

#### 2) Model checking for small state machines

**Model checking** explores all possible transitions of an abstract model to see whether properties hold. It’s most valuable when a component can be represented as a **small state machine** (modes, retries, timeouts, watchdog escalation, comms link-state, safety interlocks).

Typical workflow:

1. Write a small model of the logic (states + events + guards).
2. State properties: “unsafe state is unreachable”, “fault leads to SAFE”, “no deadlock”.
3. Run the model checker; if it finds a counterexample, turn it into:

   * a design/code fix, and
   * a regression test scenario.

Tools commonly used for this “small model” approach include:

* **TLA+ / TLC** (specification + model checking for safety/liveness properties),
* **SPIN** (Promela-based model checking, strong for concurrency),
* **CBMC** (bounded model checking for C/C++ code with assertions).

#### 3) Proving simple properties by construction

Even without a formal tool, you can “formalize” by making correctness *structural*:

* represent modes as an explicit enum + transition table,
* make illegal states unrepresentable via types,
* isolate safety-critical logic into a small, reviewable module with strict interfaces.

This reduces the state space and makes later model checking feasible.

### Where lightweight formal methods pay off most

* mode managers and safety supervisors (arming, degraded modes, fault handling),
* retry/backoff and timeout logic,
* concurrency protocols (locks, message passing, handshakes),
* configuration/version negotiation state machines,
* safety limiters and interlocks with many conditions.

### Practical adoption rule

If you can’t justify modeling a module in ~1–2 pages of states/events/invariants, it’s probably too big—split it until it’s model-checkable or contract-checkable.

---

## Testing strategy

Safety testing is strongest when it is **requirements- and hazard-driven**, with explicit attention to interfaces and coverage expectations (many regulated domains emphasize requirements-based testing plus structural coverage/analysis). ([cdn.vector.com][12])

### Unit tests for invariants and boundary cases

Focus unit tests on code that:

* enforces limits and plausibility checks,
* handles parsing/validation,
* converts units/coordinates,
* implements “safe fallback” behavior,
* handles state transitions.

Test design patterns:

* boundary values (min/max, just outside range),
* “unknown/invalid” inputs,
* monotonicity assumptions (time, sequence numbers),
* numeric overflow/underflow and saturation behavior.

### Integration tests across interfaces

Integration tests should target:

* serialization/deserialization correctness,
* protocol behavior under partial failure,
* timing contracts (timeouts, retries),
* versioning/compatibility (old config, old message formats).

Domain standards often point out that integration testing is where you find interface/relationship errors between components. ([Parasoft][13])

### System tests for hazards and safety goals

System tests should map directly to hazards:

* “When sensor X is stale, system enters degraded mode within T and commands remain within safe bounds.”
* “When comms drop, output actuators go to safe state.”

This is also where you validate end-to-end assumptions and operational constraints.

---

## Negative testing and fault injection

Negative testing asks: *“What happens when the world breaks our assumptions?”* Fault injection makes this systematic.

Fault injection is widely used in functional safety contexts as a verification activity (including industrial safety assessments), and is explicitly recommended in some domains (e.g., automotive functional safety references commonly discuss it). ([exida][14])

### Practical fault library (examples)

Inject faults at the **same boundaries where you validate inputs**:

#### Communications faults

* packet loss / duplication / reordering
* corrupted payloads / bad CRC
* delayed packets (jitter bursts)
* disconnect / reconnect storms

#### Sensor faults

* dropout (missing samples)
* frozen values (stuck-at)
* spikes/outliers
* slow drift (bias)
* stale timestamps

#### Timing faults

* timeouts at dependencies
* clock jumps (forward/backward)
* scheduler delays (CPU starvation)
* watchdog not kicked / delayed kick

#### Resource faults

* queue overflow / backpressure
* memory allocation failures (where applicable)
* file descriptor exhaustion (services)
* disk full (logging)

### How to implement without heroic effort

* Build a “fault injection layer” in test harnesses: wrappers around I/O, time, and sensor feeds.
* Run “fault campaigns” nightly: randomized but reproducible (seeded).
* When a fault reveals unsafe behavior, you should produce:

  * a bug report tied to a hazard/safety goal,
  * a regression test that reproduces the fault.

---

## Simulation and scenario-based validation

Simulation is how you scale system-level verification beyond a few hand-made tests. The key is to treat scenarios as **versioned test assets**.

### Building a scenario library for safety-critical behaviors

A good scenario library has:

* a **taxonomy** (loss of comms, degraded sensing, GNSS denial, obstacle suddenly appears, etc.),
* parameterization (distances, speeds, delays, noise levels),
* expected outcomes tied to safety goals (“must not exceed limit”, “must enter safe mode”).

In automotive/AV practice, scenario-based testing is often standardized via formats like **ASAM OpenSCENARIO** (to describe dynamic scenario content for simulation) and related OpenX standards. ([ASAM][15])

### Practical workflow

1. Start with hazards: each high-severity hazard gets multiple scenarios (nominal + degraded variants).
2. Store scenarios in version control; each scenario links to:

   * hazard IDs,
   * safety requirements,
   * expected behavior and tolerances.
3. Make scenario execution automated (CI):

   * pass/fail summary,
   * key traces saved (state transitions, safety limiters, watchdog events).
4. Promote “real incidents” into scenarios:

   * every safety-relevant bug becomes a new scenario or extends an existing one.

---

## A minimal “works for small teams” verification set

If you need a lightweight baseline:

1. Safety-focused code review checklist on every change to safety-critical modules.
2. Static analysis + type checks + “warnings are errors” for safety-critical folders.
3. Sanitizers in CI for C/C++ builds (ASan + UBSan at minimum). ([clang.llvm.org][3])
4. Unit tests for limits, invariants, and boundary conditions.
5. Integration tests for protocols + timeout/retry behavior.
6. A small fault-injection suite (dropouts + corruption + clock jumps).
7. A growing scenario library tied to hazards (even if your simulator is crude at first). ([Asam][16])

[1]: https://standards.nasa.gov/standard/NASA/NASA-STD-87398?utm_source=chatgpt.com "Software Assurance and Software Safety Standard"
[2]: https://learn.adacore.com/booklets/adacore-technologies-for-airborne-software/chapters/analysis.html?utm_source=chatgpt.com "4. Compliance with DO-178C/ED-12C Guidance: Analysis"
[3]: https://clang.llvm.org/docs/AddressSanitizer.html?utm_source=chatgpt.com "AddressSanitizer — Clang 22.0.0git documentation - LLVM"
[4]: https://clang.llvm.org/docs/UndefinedBehaviorSanitizer.html?utm_source=chatgpt.com "UndefinedBehaviorSanitizer — Clang 22.0.0git documentation"
[5]: https://electrovolt.ir/wp-content/uploads/2022/09/MISRA-C_2012_-Guidelines-for-the-Use-of-the-C-Language-in-Critical-Systems-Motor-Industry-Research-Association-2013-2013.pdf?utm_source=chatgpt.com "MISRA C:2012 Guidelines for the use of the C language in ..."
[6]: https://misra.org.uk/app/uploads/2021/06/MISRA-C-2012-Permits-First-Edition.pdf?utm_source=chatgpt.com "MISRA C:2012 Permits"
[7]: https://61508.org/wp-content/uploads/2024/11/09B-IEC-61508-Why-tool-qualification_V2_cut.pdf?utm_source=chatgpt.com "IEC 61508 – Why Tool Qualification"
[8]: https://learn.adacore.com/courses/intro-to-ada/chapters/contracts.html?utm_source=chatgpt.com "Design by contracts - learn.adacore.com"
[9]: https://lamport.azurewebsites.net/tla/tools.html?utm_source=chatgpt.com "TLA+ Tools - Leslie Lamport"
[10]: https://spinroot.com/spin/Doc/ieee97.pdf?utm_source=chatgpt.com "The Model Checker SPIN"
[11]: https://www.cprover.org/cbmc/?utm_source=chatgpt.com "CBMC: Bounded Model Checking for Software"
[12]: https://cdn.vector.com/cms/content/know-how/aerospace/Documents/Complete_Verification_and_Validation_for_DO-178C.pdf?utm_source=chatgpt.com "Complete Verification and Validation for DO-178C"
[13]: https://www.parasoft.com/learning-center/do-178c/integration-testing/?utm_source=chatgpt.com "DO-178C Integration Testing Compliance for Aerospace & ..."
[14]: https://www.exida.com/images/upload_13/ROS_11-07-062_R005_V2R1_2051_Assessment.pdf?utm_source=chatgpt.com "IEC 61508 Functional Safety Assessment"
[15]: https://report.asam.net/scenario-based-testing-on-proving-grounds?utm_source=chatgpt.com "Scenario-based Testing on Proving Grounds - ASAM eV"
[16]: https://www.asam.net/fileadmin/Standards/OpenSCENARIO/ASAM_OpenSCENARIO_BS-1-2_User-Guide_V1-2-0.html?utm_source=chatgpt.com "ASAM OpenSCENARIO: User Guide"
