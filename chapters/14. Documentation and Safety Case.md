---
title: "Core Concepts and Terminology"
permalink: /chapters/concepts/
---
# Documentation and Safety Case

### What a “safety case” is

A **safety case** is not a pile of documents. It is a **structured argument**, backed by **evidence**, intended to make a *compelling, comprehensible, valid* case that a system is acceptably safe **for a specific application in a specific operating environment**. ([ASEMS Online][1])

A useful way to think about it for software teams:

* **Safety case = the argument.**
* **Safety documentation = the supporting material** (hazard analysis, requirements, test reports, etc.).
* The safety case *points to* (and depends on) that supporting material, rather than duplicating it.

Safety cases are commonly treated as **one or more documents containing claims, arguments, and evidence** about meeting safety requirements, and they must be updated when safety-significant changes occur. ( [2])

---

### Minimal safety argument structure

#### Claims → arguments → evidence

A minimal structure is:

1. **Claim**: A statement you assert is true (e.g., “The system is acceptably safe in its defined operating domain.”).
2. **Argument**: The reasoning that breaks the claim into smaller, checkable sub-claims (often called “strategy” in notations).
3. **Evidence**: Artifacts that support each sub-claim (tests, analyses, reviews, etc.).

This “claims–arguments–evidence” style is widely used in assurance/safety cases, and is reflected in standard metamodel work (e.g., OMG’s assurance case metamodel; and common notations like CAE and GSN). ([omg.org][3])

#### A practical “minimum viable” argument skeleton

You can often get started with a top claim and 3–6 sub-claims:

**Top claim:**

* *C0: System S is acceptably safe for ODD X, with assumptions A, for hazards H.*

**Typical sub-claims:**

* *C1: Hazards are identified and risks are assessed for ODD X.*
* *C2: Safety requirements exist and are traceable to hazards and mitigations.*
* *C3: The design implements those requirements (with documented rationale for key decisions).*
* *C4: Verification provides sufficient confidence (tests, analysis, reviews).*
* *C5: Operational controls exist (monitoring, procedures, training, incident handling).*
* *C6: Change management ensures safety is preserved over updates.*

**Key “glue” you should always include:**

* **Context/ODD** (where/when the claim applies)
* **Assumptions & dependencies** (what must be true externally)
* **Definitions of “acceptably safe”** (e.g., safety goals/limits)
* **Confidence notes** (where evidence is weaker or partial)

---

### What evidence to collect

The safety case should reference evidence that is (a) relevant to the claim, (b) reviewable, (c) repeatable, and (d) kept current. In many safety case practices, the safety case is explicitly a body of argument **supported by a body of evidence**. ([ASEMS Online][1])

A software-focused evidence checklist:

#### Hazard analysis and risk

* **Hazard log** (hazard, causes, controls, residual risk, ODD/assumptions)
* Outputs of your chosen method(s): brainstorm/checklists, FMEA-style tables, STPA notes, fault trees (as applicable)
* **Risk acceptance rationale** (what “acceptable” means for this system)

#### Requirements and traceability

* **Safety requirements** derived from hazards (constraints like “must not…” and required safety functions)
* Trace links: **hazard → requirement → design element → code → test → evidence artifact**
* Change impact notes when hazards/requirements evolve

#### Design rationale (often missing, often critical)

* Architecture views of safety mechanisms (watchdogs, interlocks, limiters, degraded modes)
* Design decisions and tradeoffs (why this mitigation vs alternatives)
* Interfaces and assumptions (timing, units, validity rules, authority/override rules)

#### Verification and validation evidence

* Test results: unit (invariants/bounds), integration (interfaces), system (hazard-driven scenarios)
* Fault-injection and negative testing results (timeouts, dropped messages, corrupted inputs, clock jumps)
* Simulation/scenario library coverage (what safety-relevant scenarios you ran)

#### Reviews and static analysis

* Review records for safety-relevant changes (what was checked, by whom, outcomes)
* Static analysis reports (linters/type checks/sanitizers/rulesets), with:

  * tool versions/configs
  * baselines and suppressions justified
  * “no new issues” gating where feasible

#### Operations evidence (don’t stop at “build”)

* Monitoring/logging plan tied to safety claims (what you log to detect unsafe states)
* Incident response procedure, post-incident learning loop
* Training/checklists for operators/maintenance, if humans are in the loop

---

### Keeping docs alive

Safety cases degrade fastest when they are treated as a one-time document. Practical safety guidance treats the safety case as something you maintain as the system changes—especially for **safety-significant changes**. ( [2])

#### Practices that work for software teams

* **Docs-as-code**: keep the safety case text, hazard log, and key tables in version control.
* **Hard links to reality**:

  * link claims to specific artifacts (test report URLs, CI job artifacts, commit hashes, tagged releases)
  * keep an “evidence index” page that is mostly links, not duplicated content
* **Release gating**:

  * define a “safety evidence checklist” required to ship (tests passed, static analysis clean/baselined, hazard log updated, argument reviewed)
* **Change triggers**:

  * require a safety impact note when touching: control authority, mode logic, limits, timeouts, sensor validity rules, failsafes, updater/config, human UI flows
* **Expiration rules**:

  * mark evidence with “valid for version range” (e.g., v1.4–v1.6) so stale evidence is obvious

#### CI automation ideas (small-team friendly)

* Generate a **Safety Case Report** (HTML/PDF) from Markdown each build.
* Attach as CI artifacts:

  * test summaries/coverage
  * static analysis outputs
  * dependency and configuration manifests
* Fail CI if:

  * safety-critical modules changed but the hazard log / safety requirements file did not change
  * new static analysis findings appear (or exceed a threshold)
  * required scenario tests did not run

#### A minimal repo layout (example)

* `safety/`

  * `safety_case.md` (claims + argument outline + links)
  * `hazard_log.csv` (or YAML/Markdown table)
  * `safety_requirements.md`
  * `traceability.md` (or generated)
  * `evidence/` (index files pointing to CI artifacts)
* CI publishes versioned artifacts under the release tag.

This approach keeps the safety case aligned with what it should be: **an argument that remains true because the evidence stays current and traceable**, not because the document is beautifully written once.

[1]: https://www.asems.mod.uk/guidance/posms/smp12?utm_source=chatgpt.com "SMP12. Safety Case and Safety Case Report"
[2]: https://www.icao.int/sites/default/files/safety/pbn/External%20References/States/UK-CAA-CAP760-Guidance-on-Conduct-of-Hazard-Identif-Risk-Ass-Production-of-Safety-Cases.pdf?utm_source=chatgpt.com "CAP 760 Guidance on the Conduct of Hazard Identification ..."
[3]: https://www.omg.org/spec/SACM/2.1/PDF?utm_source=chatgpt.com "Structured Assurance Case Metamodel (SACM), v2.1"
