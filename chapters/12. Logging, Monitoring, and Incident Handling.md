## Logging, Monitoring, and Incident Handling

Logging and monitoring are “runtime evidence”: they help you (a) detect that a safety envelope is being approached or violated, (b) reconstruct what happened after an event, and (c) close the loop by updating requirements, tests, and operational procedures. In functional-safety language, many systems rely on safety mechanisms that **detect, indicate, and control faults**, including forms of **self-monitoring**. ([][1])

### What to log for safety

Safety-relevant logs are not “debug noise.” They are deliberate records of **what the system believed**, **what it decided**, and **what it did**—especially around safety boundaries and mode changes. A log management program typically defines **what must be logged, where it is stored, who can access it, and how it is reviewed** (policy + operations, not just “printfs”). ([  NIST][2])

Log these categories (prioritize determinism and reconstruction value):

**1) Events and faults (including “soft faults”)**

* Watchdog warnings/trips, deadline misses, task overruns, health-check failures
* Safety envelope violations (speed/torque/geofence), saturation/clamping events
* Sensor faults: timeouts, CRC/checksum failures, out-of-range values, NaNs, impossible jumps
* Actuator faults: command rejected, driver error, comms loss, failsafe output applied

**2) Decisions (with “reason codes”)**

* Key control decisions that change risk: entering/exiting degraded mode, switching sensor sources, disabling a feature, engaging a safety stop
* Each such decision should log:

  * decision outcome (e.g., `ENTER_DEGRADED_MODE`)
  * **reason code** (e.g., `GPS_STALE`, `VISION_DROPOUT`, `CPU_SATURATION`)
  * minimal supporting context (timestamps, sensor validity flags, health summary)

**3) Mode transitions and state-machine edges**

* Arming/disarming, autonomous/manual transitions, override engagement
* Interlock checks: which preconditions were satisfied/failed at transition time
* Any “two-step” safety actions: requested → confirmed → executed

**4) Sensor validity and data quality**

* Validity flags, freshness/age, calibration version, frame drop counts
* Time sync status (clock jumps, NTP/PTP offset out of tolerance)

**5) Operator / external overrides**

* Manual override activation, remote commands, parameter changes, safety bypass attempts (even if rejected)
* Identity/role (if applicable), interface used, and “why now” context (mission phase)

**Implementation guidance that matters in investigations**

* **Structured logs** (machine-parseable), with stable event IDs and schemas.
* **Monotonic timestamps + wall-clock** where possible; clock integrity issues should be logged.
* **Sequence numbers** on high-rate streams/events so you can prove ordering and detect gaps.
* **Tamper resistance / access control** and retention rules—logs are evidence. ([  NIST][2])

---

### On-device monitoring

On-device monitoring should answer two questions continuously:

1. **Is the system healthy right now?**
2. **Is it trending toward a safety boundary?**

A useful way to structure monitoring is to combine:

* **Service-level signals** (from SRE/operations practice): latency, traffic, errors, saturation ([sre.google][3])
* **Embedded/robotics signals**: deadline/jitter, queue backlogs, memory pressure, dropped frames/messages, thermal/power limits

**Core on-device metrics to collect and alert on**

* **Timing / performance**

  * control-loop period jitter; missed deadlines
  * per-stage latency histograms (sensor ingest → perception → planning → actuation)
  * “time-in-queue” per pipeline stage
* **Queue depth / backpressure**

  * queue sizes (current + max), oldest item age
  * rate of enqueue/dequeue, backlog growth rate
* **Memory pressure**

  * RSS/heap usage, allocation failures, GC pauses (if relevant)
  * low-memory kills, OOM events, memory fragmentation indicators (when available)
* **Dropped frames/messages**

  * counts + rate, and *where* they were dropped (driver, middleware, application)
  * “stale data used” events (more important than raw drops)
* **I/O and storage health**

  * log partition free space, write errors, fsync latency (logging must not become the cause of a hazard)
* **Health model**

  * component heartbeats, restart counts, watchdog early warnings
  * “degraded mode active” and which functions are disabled

**Alerting principles for safety**

* Alert on **trend + threshold** (e.g., queue depth rising steadily) not only “instant red.”
* Prefer alerts that map to a **safety action**: reduce rate, shed load, enter degraded mode, trigger safe stop.
* Include **fault-tolerant time interval (FTTI)-like thinking**: if your hazard requires detection+reaction within N milliseconds, monitor what threatens that budget (CPU saturation, queue growth, sensor staleness). (ISO 26262 safety mechanisms explicitly include detection/indication/control and self-monitoring concepts.) ([][1])

---

### Operational feedback loop

A safety program improves fastest when operations can report “weak signals” before harm occurs.

**Near-misses and close calls**

* OSHA defines a near-miss as an event with no damage/injury but where a slight change could have caused harm. ( [4])
* NASA’s mishap/close-call guidance emphasizes reporting and investigating close calls to identify causes, implement corrective actions, and share lessons learned. ([nodis3.gsfc.nasa.gov][5])
* Aviation SMS guidance describes safety assurance as a feedback process that uses operational data, audits, and hazard identification to adjust safety strategy and activities. ([icao.int][6])

**Make near-miss reporting usable**

* Provide a lightweight reporting path (form/template + minimal required fields).
* Encourage attaching: timestamps, log bundles, traces/metrics snapshots, configuration versions.
* Treat “false positives” (unnecessary safety stops) as reportable too—they affect trust and can create secondary hazards.

**Incident triage (practical categories)**

* **Safety incident**: safety envelope violated or harm occurred/likely.
* **Near miss**: barrier held (interlock worked, operator intervened, redundancy masked fault).
* **Anomaly**: unexpected behavior with unclear safety impact (needs investigation).
* **Nuisance / false alarm**: monitoring triggered but no credible hazard (still fix the alert logic).

**Root-cause analysis (RCA) that works for software + systems**

* Build a **timeline** from synchronized logs/metrics.
* Identify:

  * triggering event (fault/error)
  * propagations (what it affected)
  * barrier performance (what prevented escalation—or failed)
* Use structured methods as appropriate: 5-Whys, fishbone, barrier analysis; keep outputs actionable: “which guardrail was missing or mis-specified?”

---

### Post-incident actions

Incident handling should be a defined lifecycle, not an improvised meeting. NIST’s incident response model explicitly includes **preparation**, **detection & analysis**, **containment/eradication/recovery**, and **post-incident activity**. ([  NIST][7])

**Immediate actions (containment and evidence preservation)**

* Put the system into the safest feasible state (safe stop / degraded mode).
* Preserve evidence:

  * snapshot logs/metrics/traces
  * record software + configuration versions and environment details
  * prevent log loss (storage pressure, rotation policies)

**Corrective actions**

* Fix the specific cause(s) found in RCA (code defect, threshold, interface contract, configuration).
* Add/strengthen a barrier:

  * new monitor, tighter interlock, better input validation, stronger plausibility check
* Validate with targeted tests (repro scenario + regression suite) and (if you have it) simulation replay.

**Preventive actions (CAPA-style thinking)**
The FDA describes CAPA as collecting/analyzing information, investigating quality problems, and taking effective corrective/preventive action to prevent recurrence. ([U.S. Food and Drug Administration][8])
Translate that into software safety practice:

* Expand from “bug fixed” to “system improved”:

  * improve detection (new metric, better alert), reduce time-to-diagnose
  * add negative tests/fault injection for the class of failure
  * update runbooks and on-call/field procedures
  * tighten change controls on safety-critical parameters

**Evidence update**
Treat updates as part of your safety case (even if informal):

* Update hazard log / safety requirements impacted by the incident
* Add new test cases and operational monitors as **evidence** that the hazard is controlled
* Record residual risk and any new assumptions introduced (e.g., “requires time sync within X ms”)

---

### Minimal checklist (small team)

* **Logs**: mode transitions, safety actions, sensor validity, overrides, fault reason codes
* **Metrics**: latency, errors, saturation + queue depth + drops + memory pressure
* **Operations loop**: near-miss reporting path + triage rubric + RCA template
* **Post-incident**: preserve evidence, corrective fix + preventive barrier, update tests/docs/evidence

[1]: https://files.infocentre.io/files/docs_clients/126_2008096319_4226752_docu_nt_doga_ISO_26262-4.pdf?utm_source=chatgpt.com "INTERNATIONAL STANDARD ISO 26262-4 - Infocentre"
[2]: https://csrc.nist.gov/pubs/sp/800/92/final?utm_source=chatgpt.com "SP 800-92, Guide to Computer Security Log Management"
[3]: https://sre.google/sre-book/monitoring-distributed-systems/?utm_source=chatgpt.com "Chapter 6 - Monitoring Distributed Systems"
[4]: https://www.osha.gov/sites/default/files/2021-07/Template%20for%20Near%20Miss%20Report%20Form.pdf?utm_source=chatgpt.com "Near-Miss Incident Report Form"
[5]: https://nodis3.gsfc.nasa.gov/hq_lib/hqd_display.cfm?Internal_ID=HQ_PR_8621.1_Chapter1&idx=7&utm_source=chatgpt.com "CHAPTER 1. Mishap and Close Call Reporting, Investigating ..."
[6]: https://www.icao.int/sites/default/files/SMI/TrainingDocs/Chapter%209%20Safety%20Management%20Systems%20SMS/9.6-3-SMS-Guidance-Manual-2012.pdf?utm_source=chatgpt.com "SMS Guidance Manual"
[7]: https://csrc.nist.gov/pubs/sp/800/61/r2/final?utm_source=chatgpt.com "SP 800-61 Rev. 2, Computer Security Incident Handling Guide"
[8]: https://www.fda.gov/inspections-compliance-enforcement-and-criminal-investigations/inspection-guides/corrective-and-preventive-actions-capa?utm_source=chatgpt.com "Corrective and Preventive Actions (CAPA)"
