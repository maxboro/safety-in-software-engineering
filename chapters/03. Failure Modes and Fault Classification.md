---
title: "Failure Modes and Fault Classification"
permalink: /chapters/failure-modes/
---
# Failure Modes and Fault Classification

This chapter introduces a vocabulary for thinking clearly about “what went wrong,” *where* it went wrong, and *how* it can propagate into a hazardous outcome.

---

### Fault → error → failure chain

A widely used dependability taxonomy distinguishes three linked concepts: **fault**, **error**, and **failure**. 

* **Fault**: the *cause* (internal or external) that can lead the system toward incorrect behavior. Faults may be **dormant** until triggered by a particular input or condition, or **active** when they are currently producing an error. 
* **Error**: an incorrect internal state (or deviation in state) that *may* lead to a failure. Many errors are contained and never become externally visible. 
* **Failure**: an *event* where the **delivered service deviates from correct service** (what the user/system-of-systems expects). 

**Why the distinction matters in software safety:** software teams often observe the world at the *failure* level (“the actuator moved late”), debug at the *error* level (“state machine got into an impossible state”), and must ultimately eliminate or control the *fault* level (“race condition enabled by the design and scheduling assumptions”).

A key propagation idea: a fault can create an error in one component; if that error reaches the component’s interface, it can become a **service failure** of that component and then appear as a *fault to downstream components* (fault/error/failure chain across boundaries). 

---

### Failure types

#### Random/hardware-like vs systematic/software

Functional-safety standards separate two major categories because they behave differently and are handled differently:

* **Random hardware failures**: arise from **physical degradation mechanisms** and can often be modeled statistically (e.g., failure rates). ([assets.iec.ch][1])
* **Systematic failures**: arise from **specification, design, implementation, integration, procedures, or human error** (including software). They are not well captured by “failure rate” models; instead, they are reduced via lifecycle controls (requirements quality, reviews, verification, configuration management, change control). ([assets.iec.ch][1])

Practical interpretation for software developers:

* If it’s **systematic**, fixing it usually means changing the design/spec/code/process.
* If it’s **random**, you typically mitigate with diagnostics, redundancy, monitoring, derating, or replacement strategies.

#### Transient vs permanent vs intermittent

A common temporal (persistence) classification is:

* **Permanent fault**: once it occurs, it does not disappear without repair/replacement (or a design change, in the case of a software bug). ([shemesh.larc.nasa.gov][2])
* **Transient fault**: appears for a short time and then disappears (e.g., EMI/radiation upset). ([shemesh.larc.nasa.gov][2])
* **Intermittent fault**: appears, disappears, and then reappears; it can look like “flaky” behavior. ([shemesh.larc.nasa.gov][2])

Important nuance for safety work: even a transient fault can produce a **persistent error** in system state if the system is not designed to detect and recover. ([shemesh.larc.nasa.gov][2])

Also, the dependability taxonomy notes that “elusive” (hard-to-reproduce) development faults can manifest similarly to transient physical faults; this similarity is one reason intermittent/soft-error thinking shows up in real systems (especially under load/timing margins). 

---

### Common software failure modes

Below are failure modes that recur in safety-related software (embedded, robotics, industrial control, medical, aerospace). They map well to what hazard analyses often uncover.

#### Timing and real-time behavior

* **Deadline misses / late responses** (control loop misses cycle, watchdog not serviced in time).
* **Incorrect latency assumptions** (data is “fresh enough” when it isn’t; stale sensor fusion).
* **Clock/timebase issues** (bad synchronization, wrong time source, leap/rollover handling).
  NASA’s software hazard causes explicitly include timing-related issues such as incorrect latency/sampling rates, failure to complete in a given time, and incorrect time synchronization. ([swehb.nasa.gov][3])

#### Concurrency and ordering

* **Race conditions / lost updates** (shared state without proper synchronization).
* **Deadlocks / livelocks** (tasks block indefinitely or spin).
* **Priority inversion / starvation** (critical work delayed by scheduling interactions).
* **Non-deterministic startup/shutdown order** (initialization sequences differ between runs).

#### Resource exhaustion and saturation

* **Memory leaks / fragmentation** leading to allocation failures.
* **CPU overload** leading to timing failures (which can cascade into control instability).
* **Queue growth / backpressure failure** (buffers silently grow until collapse).
* **File descriptor/socket/handle exhaustion**.
  These are especially dangerous because they often start as performance issues and become safety issues when timing/availability assumptions break.

#### Numeric, units, and range problems

* **Overflow/underflow**, saturation not handled, wraparound (especially counters/time).
* **Precision/rounding errors** (control instability, threshold misfires).
* **Units mismatch** (meters vs feet, degrees vs radians).
* **Invalid domain assumptions** (sqrt of negative due to sensor glitch; NaNs propagate).

#### Configuration and parameterization

* **Wrong parameter values** (gains, thresholds, limits) for the operational context.
* **Mismatched configuration across components** (protocol versions, coordinate frames).
* **Unsafe defaults** and “debug” settings shipped to production.
  Configuration faults are explicitly called out as a broad operational fault class in the dependability taxonomy. 

#### Interfaces and integration

* **Ambiguous contracts** (what does “valid” mean? what are units? what are bounds?).
* **Schema/version drift** (producer/consumer mismatch).
* **Error handling mismatch** (one side retries, other side interprets duplicates as commands).
* **Incorrect assumptions about external systems** (sensor behavior, actuator response, network delays).

#### State machine and mode logic errors

* **Missing transitions** (system cannot reach a safe state from some modes).
* **Unsafe transitions** (switching modes without verifying prerequisites).
* **Incorrect invariants** (state says “armed” while outputs are enabled prematurely).
* **Recovery loops** (oscillation between degraded/normal modes; repeated resets).

---

### Human/organizational contributors

Many “software failures” are actually **systematic failures** rooted in the way software is specified, built, reviewed, tested, and changed.

#### Process gaps

* **Missing safety lifecycle activities** (no hazard-driven requirements, no traceability).
* **Weak change control** (patches bypass review/testing; safety arguments not updated).
* **Inadequate verification planning** (tests don’t cover timing/mode transitions/fault handling).

Functional-safety material explicitly lists systematic-failure sources such as incorrect specification, omissions in safety requirements, software errors, human error, and maintenance/modification impacts. ([assets.iec.ch][1])

#### Misunderstood or incomplete requirements

* **“Must not” behaviors not specified** (e.g., actuator must not energize on boot).
* **Assumptions not stated** (about timing, load, sensor validity, operator actions).
* **Operational scenarios ignored** (degraded modes, abnormal environments).

#### Copy-paste reuse and “proven-in-use” misunderstanding

* Reuse can import **hidden assumptions** (different timing, different units, different failure semantics).
* “It worked before” is not a safety argument unless the operating context and evidence match.

#### Inadequate reviews/testing

* Reviews focus on style/functionality but miss **hazard-focused questions** (failures under load, concurrency, sensor dropouts, command duplication).
* Tests validate “happy paths” but not **fault injection**, **stress**, **timing**, **mode confusion**, and **recovery**.

A useful mental model: systematic failures are often “designed in” (via decisions, omissions, misunderstandings) and only show up when the system enters the corner of its state space that the organization didn’t explicitly analyze or test.

---

[1]: https://assets.iec.ch/public/acos/IEC%2061508%20%26%20Functional%20Safety-2022.pdf?2023040501= "Microsoft PowerPoint - IEC 61508 & Functional Safety-2022.pptx"
[2]: https://shemesh.larc.nasa.gov/fm/papers/Butler-TM-2008-215108-Primer-FT.pdf?utm_source=chatgpt.com "A Primer on Architectural Level Fault Tolerance"
[3]: https://swehb.nasa.gov/display/SWEHBVD/8.21%2B-%2BSoftware%2BHazard%2BCauses?utm_source=chatgpt.com "8.21 - Software Hazard Causes"

### Swiss Cheese Model (defense-in-depth view of failures)

![Image](assets/swiss_model.jpg)  
[Swiss model](https://www.researchgate.net/figure/The-Swiss-cheese-model-of-human-error-causation-adapted-from-Reason-1990_fig1_247897525)  

The **Swiss Cheese Model**, originally articulated by James Reason, is a conceptual model used in safety engineering to explain **how accidents emerge from the alignment of multiple weaknesses**, rather than from a single failure.

#### Core idea

* Complex systems are protected by **multiple layers of defenses** (technical, procedural, organizational).
* Each layer has **imperfections (“holes”)** due to design limitations, implementation errors, operational pressures, or human factors.
* An accident occurs when **holes temporarily align**, allowing a hazard to pass through all layers and reach a loss.

This model is widely used in safety-critical domains (aviation, healthcare, nuclear, rail) to reason about **systemic risk**, not individual blame.

#### Mapping the model to software systems

In software-intensive systems, typical “slices of cheese” include:

* **Requirements & hazard analysis**

  * Missing hazards, incorrect assumptions, unclear constraints
* **Design**

  * Inadequate architecture for fault containment, unsafe state transitions
* **Implementation**

  * Bugs, race conditions, unchecked assumptions
* **Verification & validation**

  * Gaps in testing (no fault injection, no timing stress, no degraded-mode tests)
* **Configuration & deployment**

  * Wrong parameters, mismatched versions, unsafe defaults
* **Operations & monitoring**

  * Insufficient logging, missed alerts, unclear procedures
* **Organizational context**

  * Schedule pressure, incomplete reviews, weak change control

No single layer is expected to be perfect. Safety comes from **overlap and independence** between layers.

#### Relation to fault → error → failure

The Swiss Cheese Model complements the fault–error–failure chain:

* A **fault** may exist in one layer (e.g., a race condition in code).
* If not caught, it produces an **error** (corrupted internal state).
* If downstream layers fail to detect or mitigate it (e.g., no monitoring, unsafe recovery), the error becomes a **failure** at the system boundary.
* The model explains *why* that fault was not stopped earlier: other defenses also had holes.

#### Why this matters for software developers

For developers, the model shifts thinking from:

> “How do I eliminate all bugs?”

to:

> “How do I ensure that a bug does **not** become a hazardous outcome?”

Concrete implications:

* **Assume bugs exist** → add runtime checks, watchdogs, sanity limits.
* **Assume tests are incomplete** → design for safe defaults and fail-safe behavior.
* **Assume operators make mistakes** → make dangerous actions hard or impossible.
* **Assume future changes** → make assumptions explicit and verifiable.

#### Common misunderstandings

* **“More testing removes all holes”**
  Testing reduces holes but never eliminates them, especially for timing and concurrency.
* **“Redundancy alone is enough”**
  Redundant components with shared assumptions can fail together (common-cause failure).
* **“Human error is the root cause”**
  The model explicitly treats human error as *one layer among many*, shaped by system and organizational design.

#### Practical takeaway

When analyzing or designing software for safety, ask for each hazard:

* What layers are supposed to stop this?
* What are the known weaknesses in each layer?
* Are there **independent** defenses, or do they rely on the same assumptions?

This mindset naturally leads to **defense in depth**, better hazard analyses, and more realistic safety arguments—especially for software, where failures are almost always systematic rather than random.

