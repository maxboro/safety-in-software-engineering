
## Safety Lifecycle for Software (Practical View)

A “safety lifecycle” is the idea that safety is managed end-to-end: from concept and hazard analysis, through design/implementation and verification, into operation/maintenance, and finally decommissioning. This framing is explicit in functional-safety standards (e.g., IEC 61508) which describe lifecycle phases spanning “initial concept” through “operation and maintenance” to “decommissioning.” ([mdcpp.com][1])

![Image](assets/safety_lifecycle_iec61508.jpg)
[Safety lifecycle scheme from IEC61508](https://mdcpp.com/doc/standard/IEC61508-1-1997.pdf)  

In practice, the lifecycle is not just a sequence of phases; it also includes *cross-cutting processes* that run throughout (typical examples: planning, configuration/change control, independent review/assurance, and evidence/traceability). In avionics guidance around DO-178C, for example, lifecycle processes include development and verification plus integral processes like configuration management and quality assurance. ([ansys.com][2])

---

### Concept and requirements

#### Hazard analysis inputs, safety goals

**Goal of this phase:** establish what can go wrong (hazards), why it matters (risk), and what the system must do (or must not do) to keep risk tolerable.

**Typical inputs (what you collect before writing “safety requirements”):**

* **System definition and scope** (what is inside/outside your responsibility) and operational context/assumptions. The lifecycle framing in IEC 61508 explicitly includes concept and scope definition leading into hazard/risk analysis and safety requirements/allocation. ([library.e.abb.com][3])
* **Hazard identification + risk analysis results** (hazard log, causal factors, scenarios).
* **Operational scenarios**: normal operation, startup/shutdown, degraded modes, maintenance, misuse.
* **External dependencies**: sensors, actuators, network, cloud services, operators, third-party libraries.

**Safety goals (practical definition):**

* High-level statements derived from hazard analysis, expressing what must be achieved to prevent/mitigate unacceptable harm. In ISO 26262 terminology, “safety goals” are established from hazards identified in the concept phase and guide downstream requirements and design. ([files.infocentre.io][4])

**Work products you usually want by the end:**

* Hazard log with severity/likelihood rationale and assumptions.
* Safety goals (high-level constraints) linked to hazards.
* A first cut of safety requirements and acceptance criteria (what evidence will later show the goal is met).

---

### Architecture and design

#### Safety requirements allocation, independence, interfaces

**Goal of this phase:** turn safety goals into an implementable design where responsibilities are clearly allocated and safety does not silently depend on “nice-to-have” parts.

**Safety requirements allocation**

* Allocate safety requirements to architectural elements (software components, hardware elements, humans/procedures), and define what each element must provide.
* IEC 61508 explicitly includes “overall safety requirements” followed by “safety requirements allocation” as lifecycle steps. ([library.e.abb.com][3])

**Independence (a practical meaning for developers)**

* Separate *safety-related decisions* from non-safety functionality where feasible (e.g., independence of monitoring from the main control logic; separation of safety-critical configuration paths from UI/telemetry paths).
* Ensure independence in verification where required (e.g., someone other than the author reviews/tests critical code; independent assessment/IV&V for higher criticality projects is a common pattern in safety assurance). ([s3vi.ndc.nasa.gov][5])

**Interfaces**

* Define interfaces so that unsafe states are hard to represent:

  * explicit units, ranges, timestamps, and validity flags
  * timeouts and “freshness” rules for data
  * clear ownership of state machines and mode transitions
* Document *assumptions* each interface makes (and what happens when assumptions are violated).

**Work products you usually want by the end:**

* Safety architecture (component diagram + safety mechanisms + safe-state strategy).
* Allocated safety requirements (each with owner component and verification method).
* Interface specifications with failure behavior (timeouts, invalid data handling, degraded mode behavior).

---

### Implementation

#### Coding rules, defensive techniques, traceability

**Goal of this phase:** implement the design so common software failure modes are systematically prevented, detected, or controlled—and prove you did.

**Coding rules**

* Use a defined coding standard appropriate to your stack (e.g., MISRA-style rules in C/C++; stricter subsets/lint rules in other languages).
* Enforce rules automatically (CI gates), and document deviations with rationale.

**Defensive techniques (examples that map well to safety evidence)**

* Input validation + range checks at trust boundaries.
* Explicit state machines with guard conditions; fail-closed for invalid transitions.
* Timeouts for external dependencies; “stale data” detection.
* Resource ceilings (memory, queue sizes, CPU budgets) and controlled degradation.
* Redundant reasonableness checks (sanity checks) on sensor values and commands.
* Safe default configuration and explicit versioning of configuration.

**Traceability**

* Maintain trace links: hazard → safety goal → safety requirement → design element → code changes → verification results.
* In many safety regimes, traceability is treated as core lifecycle data because it supports change impact analysis and “show your work” evidence (this is a recurring theme across standards that emphasize lifecycle evidence and audits). ([ansys.com][2])

**Work products you usually want by the end:**

* Code + static analysis results + documented deviations.
* Traceability matrix (even if lightweight) connecting requirements to commits/tests.
* “Definition of done” for safety-related code (reviewed, analyzed, tested, traced).

---

### Verification and validation

#### Evidence collection, test strategy, reviews

**Goal of this phase:** demonstrate (with evidence) that the software satisfies its safety requirements *in its intended environment*, and that the evidence is reviewable and repeatable.

**Evidence collection**

* Treat verification outputs as *lifecycle data* you curate (test reports, review records, analysis outputs, tool versions/config, coverage, simulations, fault-injection results).
* In ISO 26262 practice, the “safety case” concept is explicitly an argument that safety is achieved, supported by evidence compiled from lifecycle work products. ([QTSI][6])

**Test strategy (practical structure)**

* **Requirement-based tests** for each safety requirement (including negative tests).
* **Scenario tests** for hazards (e.g., sensor dropouts, timing faults, corrupted messages).
* **Robustness testing**: fuzzing/parsing hardening where inputs are complex.
* **Fault injection** (simulated failures) to show detection/mitigation works.
* **Regression suite** tied to change control (every safety-relevant change triggers appropriate re-verification).

**Reviews**

* Use tiered rigor:

  * ordinary peer review for most code
  * more formal inspections for safety-critical modules (checklists, recorded findings, closure)
* Independence where required/appropriate (reviewer not the author; higher criticality may justify independent verification/validation). ([s3vi.ndc.nasa.gov][5])

**Work products you usually want by the end:**

* Verification plan + results bundle (automated + manual).
* Review records with resolved findings.
* Validation results demonstrating system-level safety goals in representative conditions.

---

### Operations and change

#### Monitoring, incident response, updates

**Goal of this phase:** keep the system safe as reality changes (usage, environment, dependencies, attackers, wear-out, operators, configurations).

Safety standards explicitly treat operation/maintenance and modification as part of the lifecycle, not “post-project work.” ([mdcpp.com][1])

**Monitoring**

* Operational telemetry targeted at safety signals:

  * watchdog resets, fault flags, degraded-mode rates, sensor validity stats, timing overruns
  * configuration versions in the field
* Alerting tuned to safety relevance (avoid alarm fatigue for operators).

**Incident response**

* Define severity categories aligned with hazards (e.g., “potential safety impact” triggers immediate escalation).
* Capture artifacts for root cause (logs with integrity protections, flight/mission traces, configs).

**Updates / change control**

* Perform **impact analysis**: what safety requirements might this change affect?
* Maintain rollback strategy and “known-safe” baselines.
* Treat third-party updates as safety-relevant changes (re-run qualification/verification as needed).
* Aviation guidance around DO-178C explicitly addresses establishing lifecycle processes and considerations for modifications and continued assurance. ([faa.gov][7])

**Work products you usually want by the end (ongoing):**

* Operational safety metrics dashboard.
* Change records with safety impact assessment + re-verification evidence.
* Field incident reports linked back into hazard log updates.

---

### Decommissioning

#### Data, configuration, handover risks

**Goal of this phase:** retire or transfer the system without creating new hazards (e.g., stale configs, orphaned services, leaked sensitive safety parameters, unsupported deployments).

IEC 61508 explicitly includes decommissioning as a lifecycle phase to be considered in functional safety management. ([mdcpp.com][1])

**Key risks**

* **Residual active behavior**: background services still issuing commands; unattended automation.
* **Configuration artifacts**: calibration tables, safety limits, allow-lists, cryptographic keys.
* **Data retention**: logs/traces containing sensitive operational details; regulatory retention needs.
* **Handover ambiguity**: unclear ownership for monitoring/patching after transfer.

**Practical controls**

* Decommission plan with checklist:

  * disable safety-relevant actuation paths
  * wipe/rotate secrets and revoke credentials
  * archive evidence and configuration needed for audits/incident investigations
  * document “last known safe versions” and known limitations
* If handing over: explicit transfer of safety assumptions, hazard log, and open safety issues.

**Work products you usually want by the end:**

* Decommission/handover report (what was turned off, removed, archived, transferred).
* Archived safety evidence bundle and configuration baseline references.
* Documented residual risks and responsibilities (who owns what after retirement).

[1]: https://mdcpp.com/doc/standard/IEC61508-1-1997.pdf?utm_source=chatgpt.com "IEC 61508 Part1-4.0"
[2]: https://www.ansys.com/simulation-topics/what-is-do-178c?utm_source=chatgpt.com "What is DO-178C?"
[3]: https://library.e.abb.com/public/23da2b648eb84d75ac3f960d626d9817/IEC%2061508%20Introduction%20End%20User.pdf?utm_source=chatgpt.com "Introduction to IEC61508 and Functional Safety"
[4]: https://files.infocentre.io/files/docs_clients/126_2008096316_3334601_ISO%2026262-1_2018.pdf?utm_source=chatgpt.com "INTERNATIONAL STANDARD ISO 26262-1 - Infocentre"
[5]: https://s3vi.ndc.nasa.gov/ssri-kb/static/resources/nasa-std-8739.8a.pdf?utm_source=chatgpt.com "NASA-STD-8739.8A"
[6]: https://www.quidditytech.io/post/why-is-safety-case-important?utm_source=chatgpt.com "Why is Safety Case important? Implications for ISO 26262 ..."
[7]: https://www.faa.gov/documentLibrary/media/Advisory_Circular/AC_20-115D.pdf?utm_source=chatgpt.com "AC 20-115D"
