---
title: "Hazard Identification Techniques"
permalink: /chapters/identification/
---
# Hazard Identification Techniques

Hazard identification is the structured activity of finding *what could lead to harm* (people, property, environment, mission) in a system, including harm that arises from software behavior, software–hardware interaction, and human/organizational operations. Many safety lifecycles start this early (e.g., as “preliminary hazard analysis”) to build an initial hazard list and candidate mitigations while architecture is still flexible. ([cto.mil][1])

## System boundary and context

Before applying any “hazard ID method,” define the *system you are analyzing* and the *context it operates in*. This prevents two common failure modes:

* analyzing only the software in isolation (missing interaction hazards), or
* analyzing “everything” (scope creep, shallow results).

### Actors, environment, external dependencies

**1) Define the system boundary**

* What is “inside” (components you control/design) vs “outside” (external services, operators, infrastructure, regulators, weather, GNSS, etc.).
* What crosses the boundary: data, control commands, power, timing signals, credentials/keys, updates, logs/telemetry.

Systems engineering practice treats boundary and interfaces as first-class artifacts because hazards frequently originate at interfaces and assumptions about the environment. ([NASA][2])

**2) Identify actors (human + non-human)**

* Human: operator, maintainer, installer, tester, bystanders, emergency responders.
* Non-human: other systems (vehicles, robots, sensors), external controllers, cloud services.

**3) Characterize environment and operating conditions**

* Physical: temperature, vibration, EMI, lighting/visibility, terrain, network coverage.
* Operational: mission phases, time pressure, workload, handovers, degraded communications.

**4) Explicitly list external dependencies**

* External APIs/services (auth, maps, telemetry relays), time sources, configuration servers, model update pipelines, build and deployment infrastructure.
* For each dependency, record: what you assume it provides (availability, integrity, latency bounds), and what happens if it violates that assumption.

**5) Define intended use, life phases, and foreseeable misuse**
Even if you don’t build “machinery,” the idea transfers well: define intended use, and also *reasonably foreseeable misuse* and life phases (commissioning, operation, cleaning/maintenance, updates, decommissioning). Safety standards for machinery explicitly include foreseeable misuse and life phases because many hazards occur outside “normal operation.” ([CEN-CENELEC][3])

Practical output: a one-page “context sheet” (boundary diagram + actors + dependencies + assumptions + life phases). This becomes the anchor for every hazard workshop.

---

## Hazard identification methods

No single method is sufficient. A common pattern is:

1. broad capture (brainstorming/checklists),
2. structured exploration (HAZOP-style guidewords, FMEA),
3. causal reasoning (FTA),
4. control/interaction-focused analysis (STPA).

### Brainstorming + checklists

**What it is**
A facilitated session to generate hazards from experience, past incidents, and known failure patterns—then organize them.

**Why it works**

* Fast coverage early in design.
* Captures domain “unknown knowns” (things experienced engineers have seen before).

**How to run it**

* Start from the context sheet (boundary, phases, actors).
* Ask: “What hazardous states can the system reach?” then “How could we get there?”
* Record hazards as *system-level statements* (e.g., “vehicle applies thrust while operator believes it is disarmed”).

**Checklist sources**
Use checklists drawn from your domain and from common software failure modes (timing, concurrency, resource exhaustion, configuration, interfaces, state machines), plus operational phases (installation, maintenance, updates). This is not about perfect completeness; it is about repeatability and reducing reliance on memory.

**Deliverable**

* Hazard log: hazard, initiating conditions, potential consequences, existing controls, candidate mitigations, assumptions.

### HAZOP-style thinking (guidewords adapted to software)

**What it is**
HAZOP is a structured “deviation from design intent” technique driven by *guidewords* applied to parts of a system. The core principle is a deliberate search for deviations from intent using guidewords. ([ITEH Standards][4])

**How to adapt to software**
In software, “design intent” often means:

* data intent (what values mean, units, ranges),
* control intent (who is allowed to command what, when),
* timing intent (deadlines, rates, ordering),
* state intent (allowed transitions).

**Example software-oriented guidewords**
Apply these to signals, APIs, messages, states, resources, and constraints:

* **NO / NOT PROVIDED**: command not sent, sensor not delivered, watchdog not kicked.
* **MORE / LESS**: rate too high/low, retries too aggressive, quota too strict/loose.
* **EARLY / LATE**: event occurs too soon/too late; stale data used.
* **AS WELL AS / EXTRA**: duplicate messages, repeated actuation, double-apply.
* **PART OF / INCOMPLETE**: partial config update, truncated payload, partial state write.
* **REVERSE / WRONG DIRECTION**: sign error, frame mismatch (NED vs ENU), swapped axes.
* **OTHER THAN**: wrong mode, wrong recipient, wrong units, wrong auth context.

**Process**

* Partition the system (e.g., “mission planning,” “command ingest,” “state estimator,” “actuation,” “update mechanism”).
* For each partition: state intent, apply guidewords, capture deviations → causes → consequences → safeguards.

**Where it shines**

* Interfaces, mode logic, sequencing, and timing assumptions—classic software hazard territory.

### FMEA / FMEDA (software-adapted use)
![FMEA](assets/FMEA.png)  
[wiki](https://en.wikipedia.org/wiki/File:FMEA.png)  
**FMEA basics**
IEC 60812 defines FMEA as a structured method to identify failure modes and their effects, used to find how items/processes might fail so treatments can be identified. ([webstore.iec.ch][5])

**Software-adapted FMEA**
Software doesn’t “wear out,” but it *systematically* fails under specific conditions. Treat “failure modes” as *credible ways the software can violate its required behavior*, for example:

* wrong output due to numeric/units conversion,
* missed deadline leading to unsafe actuation,
* concurrency race producing inconsistent state,
* resource exhaustion causing restart into unsafe mode,
* configuration mismatch after update.

For each failure mode, record:

* **Local effect** (inside component),
* **Next-level/system effect** (what the system does),
* **Detection** (how you would detect it at runtime or test time),
* **Controls** (design constraints, monitors, interlocks, safe state logic).

**FMEDA angle (diagnostics-aware)**
FMEDA extends FMEA with attention to diagnostics/detectability and (often) quantified failure rate data in hardware contexts. In software, the useful adaptation is *explicit diagnostic coverage thinking*: what failures are detectable online, which are latent, and what periodic tests/monitors exist. (FMEDA is widely used in functional safety practice even when terminology varies by standard/application.) ([r-stahl.com][6])

**Where it shines**

* Designing monitors, self-tests, plausibility checks, and safe fallback behavior.
* Making “detectability” explicit, instead of assuming “we’ll notice.”

### Fault Tree Analysis (FTA) basics

**What it is**
FTA is a top-down method: start with a defined undesired “top event” and decompose logical causes until you reach basic events you can address (design changes, controls, tests). IEC 61025 describes FTA as concerned with identifying and analyzing conditions and factors that cause or may contribute to a defined top event. ([ITEH Standards][7])

**How to use it for software**
Choose top events like:

* “Uncommanded acceleration occurs,”
* “Robot arm moves outside allowed zone,”
* “Drone enters no-fly zone.”

Then build causes using AND/OR logic:

* OR branches for alternative causal paths (e.g., “bad command accepted” OR “mode confusion” OR “state estimator fault”).
* AND branches for combinations (e.g., “stale sensor data” AND “no plausibility check” AND “controller trusts measurement”).

FTA forces clarity on:

* minimal combinations that can trigger the hazard (cut sets),
* where a single point of failure exists,
* where independence assumptions are unsafe (common cause like shared clock, shared config, shared update pipeline).

**Where it shines**

* Communicating causal structure to non-specialists.
* Identifying single points of failure and missing barriers.

### STPA basics (systems-theoretic approach)

**What it is**
STPA (Systems-Theoretic Process Analysis) is based on the idea that accidents can result not only from component failures, but also from unsafe interactions and inadequate control. STPA analyzes hazards in terms of **unsafe control actions** and the **control structure** (controllers, controlled processes, feedback). ([Flight Test Safety][8])

**Core steps (practical summary)**

1. Define accidents/losses and system-level hazards.
2. Model the control structure (who controls what, what feedback exists).
3. Identify **Unsafe Control Actions (UCAs)** using standard categories (commonly):

   * control not provided when needed,
   * control provided when not safe,
   * wrong timing/order,
   * control stopped too soon/applied too long. ([MIT][9])
4. Identify causal scenarios (why UCAs could occur): flawed process models, missing feedback, incorrect assumptions, interface issues, human factors, degraded sensors, etc.
5. Derive safety constraints (e.g., “Controller must not issue X when condition Y holds”).

**Why software teams like it**

* It targets mode logic, autonomy, human-in-the-loop control, and complex interactions—exactly where “no component failed, yet it crashed” incidents live.

**Where it shines**

* Autonomous/robotics/UAV systems, complex UI + automation, distributed control, cybersecurity-safety interactions.

---

## Operational hazards

Hazards often emerge during *use and change*, not just “steady-state operation.”

### Misuse

* Wrong mode selected under stress; misunderstood UI; automation surprise.
* Foreseeable misuse is explicitly called out in safety practice (even outside machinery it’s a valuable mindset). ([ISO][10])

Mitigations typically involve:

* mode clarity + interlocks,
* confirmation for high-energy actions,
* constraints that make unsafe actions impossible even with incorrect operator commands.

### Maintenance

* Bypassed safety checks for troubleshooting, temporary overrides left enabled, calibration errors.
* Maintenance introduces new actors and procedures—treat maintenance as a distinct operational phase in hazard ID. ([CEN-CENELEC][3])

### Updates

* Partial rollout, config drift, incompatible versions between components, safety parameters reset to defaults.
* Update mechanisms are safety-relevant “control channels” (they change behavior); include them in HAZOP/FMEA/STPA partitions.

### Degraded modes

Operating with reduced capability can be safer than full shutdown in some contexts (e.g., maintaining minimal control). This “degraded mode” idea is explicitly recognized in resilience guidance (even in security-focused standards) as an alternative to total shutdown. ([csf.tools][11])

Hazard ID questions for degraded mode:

* What functions must remain to avoid harm?
* What must be disabled to prevent unsafe actuation?
* How is degraded mode entered/exited, and can it oscillate?

### Partial failures (especially in distributed systems)

* One node restarts; others continue with stale assumptions.
* Network partitions produce inconsistent worldviews.
* Sensors partially fail (stuck-at, biased, delayed) without a clean “offline” signal.

Apply:

* HAZOP guidewords (“late,” “other than,” “incomplete”) to messages and time,
* FMEA to “monitoring failures” (failure to detect partial failure is itself a failure mode),
* STPA to missing/incorrect feedback and incorrect control actions.

---

## Minimal practical workflow for a small team

1. Create the **context sheet** (boundary, actors, dependencies, assumptions, life phases).
2. Run **brainstorming + checklist** → initial hazard log.
3. Do a **HAZOP-style pass** on interfaces, modes, timing, configuration/update path (guidewords).
4. Do **FMEA** on critical software functions (include detection/diagnostics thinking).
5. Build **FTA** for 2–5 highest-severity top events to reveal single points of failure.
6. If autonomy/complex interaction is present, do **STPA** on the control structure and UCAs.

This combination is usually more effective than trying to force one technique to do everything.

[1]: https://www.cto.mil/wp-content/uploads/2025/07/MIL-STD-882E-w_CHANGE-1.pdf?utm_source=chatgpt.com "department of defense standard practice system safety"
[2]: https://www.nasa.gov/wp-content/uploads/2018/09/nasa_systems_engineering_handbook_0.pdf?utm_source=chatgpt.com "NASA Systems Engineering Handbook"
[3]: https://www.cencenelec.eu/media/CEN-CENELEC/Areas%20of%20Work/CENELEC%20sectors/Mechanical%20and%20Machines/Documents/Quicklinks/eniso12100relationmachinerydirective.pdf?utm_source=chatgpt.com "EN ISO 12100 and its relation to the Machinery Directive"
[4]: https://cdn.standards.iteh.ai/samples/20730/64bc0f1bde424e8083959f2d3c4567a2/IEC-61882-2016.pdf?utm_source=chatgpt.com "IEC 61882:2016"
[5]: https://webstore.iec.ch/en/publication/26359?utm_source=chatgpt.com "IEC 60812:2018"
[6]: https://r-stahl.com/fileadmin/tx_aimeos/Files/1_/01/STAHLQ1807006R035_001_01/FMEDA-Report_STAHLQ1807006R035_001_01.pdf?utm_source=chatgpt.com "FMEDA - Failure Modes, Effects and Diagnostic Analysis"
[7]: https://cdn.standards.iteh.ai/samples/12812/877551080d2e42c3b1241586fa45d9c0/IEC-61025-2006.pdf?utm_source=chatgpt.com "IEC INTERNATIONAL 61025 STANDARD"
[8]: https://www.flighttestsafety.org/images/STPA_Handbook.pdf?utm_source=chatgpt.com "NANCY G. LEVESON JOHN P. THOMAS"
[9]: https://psas.scripts.mit.edu/home/wp-content/uploads/2014/03/Systems-Theoretic-Process-Analysis-STPA-v9-v2-san.pdf?utm_source=chatgpt.com "Systems Theoretic Process Analysis (STPA) Tutorial"
[10]: https://www.iso.org/obp/ui/?utm_source=chatgpt.com "Safety of machinery — General principles for design"
[11]: https://csf.tools/reference/nist-sp-800-53/r5/au/au-5/au-5-4/?utm_source=chatgpt.com "AU-5(4): Shutdown on Failure"

