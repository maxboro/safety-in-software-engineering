---
title: "Introduction to Software Safety"
permalink: /chapters/introduction/
---
# Introduction to Software Safety
## Why software can be a safety issue

Software becomes a safety concern when it can *influence hazardous energy or decisions*—directly (commanding actuators) or indirectly (informing humans/automation). In many systems, safety depends on “functional safety”: correct behavior in response to inputs, including behavior under fault conditions. Standards commonly define safety as *freedom from unacceptable risk* and define functional safety as the part of safety that depends on a system operating correctly in response to its inputs. ([VDE Verlag][1])

Typical ways software contributes to accidents:

* **Wrong control action**: e.g., software commands motion/valves/energy when it should not.
* **Missing control action**: e.g., fails to brake/stop/close on demand.
* **Timing and sequence errors**: correct logic, wrong time (deadlines missed, race conditions).
* **Mode confusion**: system is in a different mode than the operator believes (UI/UX + state logic).
* **Bad assumptions about environment**: sensor limitations, edge cases, degraded conditions.
* **Systematic failures**: unlike random hardware faults, software failures are often design/requirements/interaction issues; functional-safety framing explicitly includes systematic failures such as software. ([IEC][2])

## Safety vs quality attributes

### Safety vs reliability

* **Reliability** is about “how often something fails.”
* **Safety** is about “whether failures (or behaviors) can lead to unacceptable harm.”

A system can be reliable but unsafe (it consistently does the wrong thing in a rare scenario), or safe but not very reliable (it trips into a safe state too often). A concise way to ground this is: safety is defined in terms of *unacceptable risk*, which depends on both likelihood and severity of harm—so reliability helps, but is not sufficient by itself. ([VDE Verlag][1])

Practical implication for developers:

* Don’t treat “low bug rate” as a safety argument. You still need hazard-driven constraints (what must never happen, even if everything else fails).

### Safety vs security (and why they interact)

Security is commonly defined around **confidentiality, integrity, and availability** (CIA). ([  NIST][3])

Safety and security interact because:

* **Security failures can become safety failures**: an attacker (or malware) can cause unsafe commands, disable interlocks, or corrupt sensor data (integrity/availability problems).
* **Safety mechanisms can create security exposure** if poorly designed (e.g., debug backdoors, unauthenticated maintenance modes).
* **Shared dependencies** matter: networks, updates, time sources, and identity/auth systems can be on the critical path for safe operation.

Developer takeaway:

* Treat “integrity of commands and sensor data” and “availability of safety functions” as both security and safety concerns. ([  NIST][3])

### Safety vs availability / performance

Availability and performance are often product goals, but safety can require *intentional reduction* of availability/performance:

* **Fail-safe behavior** may stop the system (reduced availability) to prevent harm.
* **Safety envelopes/limits** may cap speed/torque/throughput (reduced performance) to keep risk tolerable.
* **Real-time performance is safety-relevant** when timing itself is a hazard contributor (missed deadlines can be equivalent to wrong decisions).

Developer takeaway:

* Make “what happens under overload” explicit: load shedding, degraded modes, bounded queues, and deterministic timeouts are safety design choices—not just performance tuning.

## Where safety engineering applies

Safety engineering applies anywhere software can contribute to hazardous outcomes. Common domains and typical framing:

* **Industrial automation / process plants**: safety instrumented functions, emergency shutdown, interlocks (often under the IEC 61508 family umbrella concept for functional safety). ([IEC][2])
* **Medical devices and medical software**: regulated lifecycle expectations for medical device software processes (IEC 62304). ([webstore.iec.ch][4])
* **Automotive**: hazards due to malfunctioning behavior of E/E systems are a core concern (ISO 26262). ([iso.org][5])
* **Aviation**: software assurance objectives tied to airworthiness; DO-178C is widely referenced as a primary means to obtain approval for airborne software. ([RTCA][6])
* **Rail**: safety-related software for railway control/protection has dedicated standards (e.g., EN 50128 / IEC 62279). ([verifysoft.com][7])
* **Robotics, UAVs, infrastructure**: often combine real-time control, autonomy, human interaction, and operational complexity; safety work is driven by hazard severity, autonomy level, and regulatory/customer expectations (even when the domain standard varies).

## The safety mindset

### System thinking

Safety is an **emergent property** of the whole system: software + hardware + humans + environment + procedures. A “correct” component can still be unsafe in an unsafe interaction.

Practical habits:

* Always ask: *What are the hazards? What control actions can cause them? What assumptions must hold?*

### “Assume failure”

Start from the premise that components will fail or behave unexpectedly (including software logic, sensors, comms, time sources). Safety is about remaining acceptable under those failures—often by detecting problems and moving to a safer state. This aligns with functional-safety framing and software-safety guidance that emphasizes verifying safety behavior within the specified operating environment. ([IEC][2])

Practical habits:

* Design for bad inputs, stale data, partial outages, and timing jitter.
* Prefer explicit state machines and defined degraded modes over “best effort.”

### Human factors

Humans are part of the system: operators, maintainers, and users can misunderstand, be overloaded, or make mistakes. Safety engineering treats these as normal conditions to design for, not as “user error.” Workplace safety guidance repeatedly notes that human failure contributes to the majority of accidents/incidents. ([hse.gov.uk][8])

Practical habits:

* Make modes visible, alerts actionable, and unsafe actions hard to perform accidentally.
* Ensure logs, overrides, and procedures support correct recovery under stress.

### Operational context

Safety depends on the real operating environment: maintenance, updates, misconfiguration, degraded sensors, unusual weather/lighting, network partitions, etc.

Practical habits:

* Write down the operational assumptions (ODD where applicable), and define what the system does when assumptions are violated.
* Treat updates and configuration changes as safety-relevant changes (they can invalidate prior safety evidence).

[1]: https://www.vde-verlag.de/iec-normen/preview-pdf/info_isoiecguide51%7Bed1.0%7Den.img.pdf?utm_source=chatgpt.com "GUIDE 51"
[2]: https://assets.iec.ch/public/acos/IEC%2061508%20%26%20Functional%20Safety-2022.pdf?2023040501=&utm_source=chatgpt.com "Overview of IEC 61508 & Functional Safety"
[3]: https://csrc.nist.gov/glossary/term/security?utm_source=chatgpt.com "Glossary | CSRC - NIST Computer Security Resource Center"
[4]: https://webstore.iec.ch/en/publication/6792?utm_source=chatgpt.com "IEC 62304:2006"
[5]: https://www.iso.org/obp/ui/?utm_source=chatgpt.com "ISO 26262-1:2011(en), Road vehicles — Functional safety"
[6]: https://www.rtca.org/do-178/?utm_source=chatgpt.com "DO-178() Software Standards Documents & Training"
[7]: https://verifysoft.com/en_EN_50128_Software_for_Railway_Control_and_Protection_Systems.html?utm_source=chatgpt.com "EN 50128 \"Railway applications"
[8]: https://www.hse.gov.uk/humanfactors/topics/humanfail.htm?utm_source=chatgpt.com "Managing human failures: Overview"
