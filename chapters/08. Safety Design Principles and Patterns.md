
# Safety Design Principles and Patterns

This chapter focuses on practical, reusable design principles that reduce the probability of hazards and limit the consequences of failures. The core idea is the same across domains (industrial, automotive, medical, robotics): define what “safe” means **in context**, then build **layers of prevention, detection, containment, and recovery** around the safety-critical functions.

---

## Safe state and safe transition

### Defining “safe” per system and context

A **safe state** is not a universal property like “power off”. It is a system mode that avoids an **unreasonable level of risk** for the relevant operational context. ISO 26262 explicitly uses this framing (safe state = “mode of operation, without an unreasonable level of risk, of the item”). ([ResearchGate][1])

Practical implications:

* Safe state must be defined **per hazard scenario**, not once for the whole product.
* Safe state may be:

  * **Stopped** (e.g., robot torque disabled and brakes engaged),
  * **Limited operation** (e.g., speed/force caps),
  * **Human takeover** (e.g., controlled handover with clear annunciation),
  * **Isolation** (e.g., shut a valve; de-energize an actuator),
  * **Containment** (e.g., “hold last safe command” vs “zero command” depends on dynamics).

### Safe transition (getting to safe in time)

Safety often depends on reaching the safe state **fast enough** after a fault. That drives:

* **Detection time** (how quickly you notice something is wrong),
* **Reaction time** (how quickly you switch modes / disable outputs),
* **System inertia / energy** (how long it remains hazardous after you command “stop”).

Patterns that operationalize safe transitions:

* **Safety monitor + safety action**: monitor checks invariants; action forces safe outputs.
* **Interlocks**: prevent unsafe transitions (e.g., cannot arm motors unless all preconditions true).
* **Latched faults**: once a safety fault is detected, require an explicit reset + rechecks.
* **Timed transitions**: if safe state requires steps (e.g., slow-stop then brake), enforce deadlines per step.

---

## Fail-safe, fail-operational, fail-silent, fail-secure

These terms are often mixed; treat them as **design intents** tied to a specified “safe/secure condition,” not as labels you can apply without a scenario.

### Fail-safe

In NIST’s glossary, **fail safe** is a termination mode that prevents damage to specified resources/entities (including life/property), even though a failure might still lead to a security compromise. ([Центр безопасности NIST][2])
Interpretation: if something goes wrong, the system’s behavior should bias toward **preventing harm**, typically by transitioning to a defined safe state.

Examples (scenario-dependent):

* Motor control fault → disable torque + apply brake.
* Sensor implausibility → stop autonomous actuation and alert operator.

### Fail-operational

**Fail-operational** means continuing operation after certain faults, often using redundancy and reconfiguration. A common characterization in fault-tolerant control literature is: operate with no change in objectives/performance despite a single failure (or continue safely with defined degradation). ([backend.orbit.dtu.dk][3])
Practical use: when stopping immediately is itself unsafe (e.g., some aircraft/vehicle functions, critical medical support), you design to keep the safety function available through faults.

### Fail-silent

A classic definition: a **fail-silent node** is self-checking and “either functions correctly or stops functioning after an internal failure is detected.” ([computer.org][4])
This is common in distributed safety systems: better to go silent than emit potentially wrong commands/data that downstream components might trust.

Typical implementation techniques:

* **Self-tests + comparison** (lockstep / redundant computation),
* **Heartbeat/timeout supervision** by peers,
* **Output gating**: block outputs unless checks pass.

### Fail-secure

NIST defines **fail secure** as termination that prevents loss of the secure state when a failure occurs or is detected. ([Центр безопасности NIST][5])
Safety vs security can conflict: emergency exits are usually fail-safe (unlock on power loss), but secure areas may require fail-secure (remain locked). Resolve via **explicit prioritization** (life safety usually dominates) and engineered exceptions (e.g., fire alarm overrides).

---

## Defense in depth

Defense in depth is layered controls so that if one control fails or is bypassed, another still protects the safety objective. NIST defines defense-in-depth as a multi-layer strategy integrating people/technology/operations across multiple layers. ([Центр безопасности NIST][6])

Applied to safety design, think in layers:

1. **Inherently safer design** (eliminate hazards where possible).
2. **Prevention** (constraints, interlocks, limits).
3. **Detection** (diagnostics, plausibility checks, monitors).
4. **Containment** (isolation boundaries, partitioning, output gating).
5. **Mitigation** (fallback control, safe stop, barriers).
6. **Operational controls** (procedures, training, maintenance).
7. **Evidence and monitoring** (logging, health telemetry, incident response hooks).

Key rule: *layers should be as independent as practical* (to avoid common-cause failures).

---

## Redundancy and diversity

### Hardware/software redundancy, design diversity, voting

Redundancy improves tolerance to faults by providing alternative ways to perform a function. The simplest form is replication + selection:

* **N-modular redundancy** with **majority voting** (e.g., Triple Modular Redundancy: 3 channels + voter, 2-out-of-3 wins). ([Википедия][7])
* **Hot standby / warm standby / cold standby** depending on how fast you must recover.

For software, **N-version programming** is a classic diversity approach: multiple independently developed functionally-equivalent versions run in parallel; if failures are independent, the probability of identical wrong outputs is reduced. ([scispace.com][8])

### The trap: common-cause failure

Redundancy can fail if replicas share the same weakness:

* same spec ambiguity,
* same algorithmic assumption,
* same compiler/runtime bug,
* shared power/clock/network,
* shared bad sensor input.

That’s why **diversity** matters:

* different implementations, teams, toolchains,
* different sensing modalities (e.g., camera + radar; encoder + current sensing),
* different physical routing / power domains.

Voting patterns (and cautions):

* **2oo3** (two-out-of-three) majority voting: masks single fault but adds voter criticality.
* **Comparison + shutdown**: if mismatch, stop outputs (leans fail-silent).
* **Weighted voting**: only with strong justification; can hide faults if weights are wrong.

---

## Fault containment

Fault containment is about ensuring faults don’t propagate into system-wide hazards.

A concrete operational concept is a **Fault Containment Region (FCR)**. JPL describes an FCR as a segment designed so that faults inside it do not propagate and cause irreversible damage beyond its limits (including subtle propagation like contention/interference). ([Indico at ESA / ESTEC (Indico)][9])

### Isolation boundaries, partitioning, sandboxing

Common containment boundaries:

* **Process boundaries** (separate address spaces),
* **Memory protection** (MPU/MMU),
* **Time partitioning** (CPU scheduling isolation),
* **Communication boundaries** (validated messages, rate limits, CRC, sequence checks),
* **Power domains** (brownout isolation),
* **Privilege boundaries** (least privilege; restricted syscalls).

Aviation-style partitioning is an explicit safety mechanism: **ARINC 653** uses **spatial and temporal partitioning** to isolate applications on the same platform. ([ResearchGate][10])

Sandboxing pattern (software-focused):

* Run non-safety-critical or less-trusted components in a restricted environment.
* Only allow interaction via **narrow, validated interfaces**.
* Failures lead to **restart/isolation** of the sandbox, not corruption of safety logic.

Design checklist:

* Define FCRs early (what can fail together? what resources are shared?).
* Ensure safety monitors are *outside* the region they monitor, when possible.
* Treat shared infrastructure (logger, IPC bus, time sync, allocator) as potential fault propagators.

---

## Graceful degradation

Graceful degradation is the ability to continue operating with **limited functionality** rather than catastrophic failure, to prevent worst-case outcomes. ([TechTarget][11])

### Reduced capability modes, degraded sensing, fallback control

In safety systems, degraded modes must be **pre-designed**:

* Which functions are shed first?
* What minimum set must remain to maintain safety?
* What operator indications and constraints apply?

Common patterns:

* **Degraded sensing**: switch to lower-accuracy but trustworthy estimation; increase safety margins.
* **Fallback control**: replace advanced control with simpler proven control.
* **Capability caps**: reduce speed, force, acceleration, workspace, autonomy level.
* **Minimal risk condition**: controlled stop/park/hold position, depending on domain.

A widely referenced safety pattern for fallback is the **Simplex architecture**: switch control authority from an advanced (potentially unverified) controller to a simpler baseline controller when runtime checks detect risk, preserving safety. ([CMU Software Engineering Institute][12])

Critical nuance: graceful degradation must not become “limp but unpredictable.” You still need:

* clear rules for entering/exiting degraded modes,
* bounded behavior (limits),
* operator awareness,
* test coverage for degraded paths (they’re often under-tested).

---

## Time and scheduling safety

Timing faults are a major source of software hazards: missed deadlines, stuck tasks, queue buildup, priority inversion, or runaway loops.

### Deadlines, watchdogs, timeouts, rate limiting

**Watchdogs**
A watchdog timer asserts recovery action (often reset) if it does not receive periodic service within a specified time window. ([TI][13])
IEC 61508-related practice explicitly treats watchdog-based program sequence monitoring as a diagnostic measure (implementation guidance commonly references IEC 61508-2 in this context). ([Analog Devices][14])

Watchdog patterns:

* **External watchdog** (more independent) supervising the main MCU.
* **Windowed watchdog** (detects both “too slow” and “too fast,” catching runaway loops).
* **Hierarchical watchdogs**: task-level supervision → system supervisor → hardware watchdog.

**Timeouts**
Timeout every operation that can block:

* IPC calls, network requests, sensor reads, actuator acknowledgements, mutex locks.
  Timeouts must have defined outcomes:
* retry, switch to degraded mode, or safe stop.

**Rate limiting and backpressure**
Prevent overload cascades:

* cap message rates per producer,
* bounded queues (drop policy must be safety-justified),
* shed non-critical work first,
* admission control (don’t start work you can’t finish before deadlines).

**Temporal isolation**
If mixed criticality workloads share a CPU, use time partitioning/priority schemes so non-critical tasks can’t starve safety tasks. ARINC 653-style temporal partitioning is a canonical model. ([ResearchGate][10])

Practical checklist for developers:

* Define *timing requirements* as safety constraints (e.g., “fault must be detected within X ms”).
* Implement deadlines at the boundary (sensor read → validated state estimate available → actuator command issued).
* Test timing under stress (CPU load, memory pressure, IO delays).
* Treat “performance optimization” changes as potentially safety-relevant because they can change schedules and watchdog margins.

---

## Patterns summary (quick reference)

* **Safe state + safe transition**: define per hazard scenario; enforce timed transitions. ([ResearchGate][1])
* **Fail-safe / fail-secure**: clarify whether you’re protecting life/property vs secure state. ([Центр безопасности NIST][2])
* **Fail-silent**: stop outputs when correctness can’t be assured. ([computer.org][4])
* **Fail-operational**: continue safety function via redundancy/reconfiguration. ([backend.orbit.dtu.dk][3])
* **Defense in depth**: layered, partially independent controls. ([Центр безопасности NIST][6])
* **Redundancy + diversity + voting**: replicate and compare, but manage common-cause failures. ([Википедия][7])
* **Fault containment**: explicit regions and boundaries; partitioning for isolation. ([Indico at ESA / ESTEC (Indico)][9])
* **Graceful degradation / fallback**: pre-designed degraded modes; Simplex-style fallback where applicable. ([TechTarget][11])
* **Time safety**: watchdogs, timeouts, rate limits, and temporal isolation. ([TI][13])

[1]: https://www.researchgate.net/profile/Rami-Debouk/publication/331650722_Overview_of_the_2nd_Edition_of_ISO_26262_Functional_Safety-Road_Vehicles/links/5c8660fc299bf16918f61f43/Overview-of-the-2nd-Edition-of-ISO-26262-Functional-Safety-Road-Vehicles.pdf?utm_source=chatgpt.com "Overview of the 2nd Edition of ISO 26262: Functional Safety"
[2]: https://csrc.nist.gov/glossary/term/fail_safe?utm_source=chatgpt.com "fail safe - Glossary - NIST Computer Security Resource Center"
[3]: https://backend.orbit.dtu.dk/ws/files/138073518/safeprocess_02h.pdf?utm_source=chatgpt.com "What is Fault Tolerant Control"
[4]: https://www.computer.org/csdl/journal/tc/1996/11/t1226/13rRUyYjK4s?utm_source=chatgpt.com "Implementing Fail-Silent Nodes for Distributed Systems"
[5]: https://csrc.nist.gov/glossary/term/fail_secure?utm_source=chatgpt.com "fail secure - Glossary | CSRC"
[6]: https://csrc.nist.gov/glossary/term/defense_in_depth?utm_source=chatgpt.com "defense-in-depth - Glossary | CSRC"
[7]: https://en.wikipedia.org/wiki/Triple_modular_redundancy?utm_source=chatgpt.com "Triple modular redundancy"
[8]: https://scispace.com/pdf/diversity-through-n-version-programming-current-state-1gsxz37o1a.pdf?utm_source=chatgpt.com "Diversity Through N-Version Programming: Current State, ..."
[9]: https://indico.esa.int/event/62/contributions/2808/attachments/2356/2717/0905_-_fault-management-at-jpl_Presentation.pdf?utm_source=chatgpt.com "Fault Management at JPL: Past, Present and Future"
[10]: https://www.researchgate.net/profile/Jose-Rufino/publication/228586215_ARINC_653_interface_in_RTEMS/links/5a40db930f7e9ba8689ee673/ARINC-653-interface-in-RTEMS.pdf?utm_source=chatgpt.com "ARINC 653 INTERFACE IN RTEMS"
[11]: https://www.techtarget.com/searchnetworking/definition/graceful-degradation?utm_source=chatgpt.com "What is graceful degradation? | Definition from TechTarget"
[12]: https://www.sei.cmu.edu/documents/1146/1996_005_001_16463.pdf?utm_source=chatgpt.com "An Architectural Description of the Simplex Architecture"
[13]: https://www.ti.com/document-viewer/ja-jp/lit/html/SSZTAH7?utm_source=chatgpt.com "What Is a Watchdog Timer and Why Is It Important?"
[14]: https://www.analog.com/en/resources/analog-dialogue/articles/improving-industrial-functional-safety-part-4.html?utm_source=chatgpt.com "Program Sequence Monitoring Using Watchdog Timers"
