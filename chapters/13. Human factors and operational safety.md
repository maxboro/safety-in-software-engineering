# Human Factors and Operational Safety

Safety-critical software is almost never “just software”: it is software + people + procedures + an operational environment. Many real-world failures are not pure algorithm bugs; they are *coordination failures* between humans and automation, especially under time pressure, workload, interruptions, or unclear system states. ([NASA  ][1])

## Humans in the loop

### Operator workload: design for the worst minute, not the average hour

Operators tend to be calm for long periods and then suddenly overloaded (abnormal situation, multiple alarms, unclear fault, degraded sensing). Human-factors guidance for automation highlights that interrupting users at high-workload or critical moments can delay response and worsen outcomes—so automation should be careful about *when* and *how* it demands attention. ([hf.tc.faa.gov][2])

Practical implications for developers:

* **Make “high workload” explicit in the software model.** Examples: emergency stop active, multiple faults, comms loss, degraded mode, operator manually controlling. Use that state to throttle non-critical notifications and UI changes.
* **Rate-limit and prioritize alerts.** Too many alarms create *alarm floods* (operator overload, loss of prioritization). This is recognized in industrial alarm-system management guidance. ([hse.gov.uk][3])
* **Design for “one glance” comprehension** in abnormal states: what happened, what mode you’re in, what the system is doing, and what action is expected.

### Alerts: avoid “alarm fatigue” and missed critical events

High exposure to alarms can desensitize operators, increasing the chance of missed or delayed response (“alarm fatigue”). While the term is prominent in healthcare, the mechanism generalizes to many operational contexts: excessive false/low-value alarms reduce trust and attention for the alarms that matter. ([NCBI][4])

Concrete practices:

* **Classify alerts** (info/warning/critical) with strict rules for each class (UI prominence, sound, escalation, logging).
* **Make alarms actionable**: include cause hypothesis, confidence/sensor validity, and the *next safe step*.
* **Track alarm quality**: false positive rate, duplicates, time-to-acknowledge, time-to-resolve, and “alarm storms” per incident.

### Automation surprises and “out-of-the-loop” errors

With higher automation, people often lose situation awareness and become worse at taking over when the automation fails or reaches its limits. Human-automation research discusses *automation surprises* and the “out-of-the-loop” problem as recurring risks. ([Massachusetts Institute of Technology][5])

Developer checklist for preventing surprises:

* **Make automation intent visible**: show what the system *believes*, what it *will do next*, and why (key inputs, constraints, thresholds).
* **Expose automation limits**: explicitly signal when assumptions are violated (sensor invalid, map mismatch, low confidence, outside ODD).
* **Design takeover**: clear handoff prompts, stable controls during takeover, and predictable behavior if the operator does nothing.
* **Avoid “silent autonomy”**: no unannounced mode changes; no hidden fallback behaviors that look like normal operation.

## UI/UX safety basics

### Clear modes and transitions: prevent mode confusion

Mode confusion is a classic source of operator error: the person thinks the system is in one mode while it is actually in another. Automation guidance emphasizes coordination breakdowns when system state/behavior is not transparent. ([NASA  ][1])

UI rules that help in safety-relevant systems:

* **Single source of truth for mode**: one prominent mode indicator, consistent vocabulary, consistent placement.
* **Explicit transition cues**: “Armed → Active”, “Auto → Manual”, “Degraded sensing enabled”, with time and reason.
* **No ambiguous states**: avoid “Auto-ish” behavior where the UI says manual but hidden automation still intervenes.

### Confirmations and safeguards: stop accidental dangerous actions

Use confirmations when an action is irreversible or has high hazard potential, but avoid “confirmation fatigue”:

* Prefer **two-step actions with context** (e.g., “Hold to engage”, “Type the mode name to confirm”).
* Confirmations should explain **consequence + safer alternative** (e.g., “This disables collision avoidance. Use Degraded Mode instead.”).
* For frequent operations, use **interlocks** rather than repeated popups (preconditions, permissions, arming states).

### Prevent dangerous confusion

Common UI failure patterns to actively design out:

* **Similar-looking statuses with different meanings** (e.g., “OK” vs “Valid”, “Connected” vs “Healthy”).
* **Overloading colors** (red/yellow/green reused inconsistently).
* **Unit ambiguity** (m/s vs km/h; degrees vs radians; local vs global frames).
* **Mixed responsibility** (unclear whether the operator or the automation is currently responsible for safety-critical control).

## Procedures and training

Procedures and training are part of the safety control set: they reduce risk when the system is complex, failure-prone, or used under stress. Security standards also treat training as an expected organizational control (initial training, after system changes, periodic refresh). ([NIST  ][6])

### Checklists (operational and maintenance)

Use checklists when:

* Steps are sequential and skipping one can create hazard.
* The operation is rare (emergency procedures, unusual maintenance).
* The system is in a degraded or abnormal state.

Checklist design tips:

* **Short, scenario-triggered checklists** (“If GPS invalid”, “If actuator saturation”, “If comms loss”).
* Include **go/no-go criteria** and **safe abort points**.
* Tie checklist steps to **observable system signals** (UI indicators, logs, sensor validity flags).

### Safe maintenance and updates

Maintenance is a high-risk phase: guards are disabled, sensors are disconnected, configs are changed. Functional safety guidance explicitly notes that humans may be considered part of a safety-related system and that human factors matter at the interfaces to safety-related functions. ([exida][7])

Software patterns to support safe maintenance:

* **Maintenance mode** with constrained capabilities and loud indicators.
* **Configuration schema validation + versioning**, plus “known-good” rollback.
* **Post-maintenance self-check** with explicit pass/fail and blocked activation on failure.

### Emergency response

Have a minimal “what to do right now” runbook:

* Clear triggers (what constitutes an emergency / near-miss).
* Roles (“operator”, “on-call engineer”, “safety lead”).
* Evidence capture (logs, sensor dumps, UI state, timeline).
* Practice via periodic drills; incident-response training is a recognized control category. ([csf.tools][8])

## Misuse and adversarial environment (light touch)

### When security becomes safety-relevant

Security failures can create safety hazards: spoofed sensor data, unauthorized commands, denial-of-service that causes missed deadlines, malicious configuration changes, or compromised update channels. Cybersecurity engineering standards exist specifically for domains where cyber risk can translate into physical risk (e.g., road vehicles), and they are positioned to complement functional safety practice. ([iso.org][9])

Developer practices that connect security to safety (without turning this into a full security chapter):

* **Identify safety-critical assets and channels** (commands, configs, time sync, sensor streams) and enforce strong integrity/authentication.
* **Treat suspicious input like sensor faults**: quarantine, degrade safely, alert with high priority.
* **Design safe behavior under attack-like conditions** (loss of network, delayed messages, clock jumps): fail into a bounded safe envelope rather than “best effort” control.
* **Exercise incident paths**: integrate security incidents into operational drills and runbooks. ([csf.tools][8])

## Practical “done this week” checklist (small-team friendly)

* Add explicit system states: Normal / High workload / Degraded / Manual takeover / Maintenance.
* Implement alert prioritization + rate limiting; measure false alarms and alarm storms. ([hse.gov.uk][3])
* Make automation intent visible (what it thinks, what it will do next, why) and log every mode transition. ([NASA  ][1])
* Write 3 short checklists: “Degraded sensing”, “Unexpected behavior”, “Post-maintenance validation”.
* Add one drill: “simulate comms loss + recovery” and verify operators can diagnose state within 30–60 seconds using UI + logs. ([hf.tc.faa.gov][2])

[1]: https://ntrs.nasa.gov/api/citations/19980016965/downloads/19980016965.pdf?utm_source=chatgpt.com "/?i,.I_L-"
[2]: https://hf.tc.faa.gov/hfds/download-hfds/hfds_pdfs/Ch3_Automation.pdf?utm_source=chatgpt.com "Chapter 3 Automation"
[3]: https://www.hse.gov.uk/comah/sragtech/techmeascontsyst.htm?utm_source=chatgpt.com "Control systems"
[4]: https://www.ncbi.nlm.nih.gov/books/NBK555522/?utm_source=chatgpt.com "Alarm Fatigue - Making Healthcare Safer III - NCBI"
[5]: https://web.mit.edu/16.459/www/parasuraman.pdf?utm_source=chatgpt.com "Humans and Automation: Use, Misuse, Disuse, Abuse"
[6]: https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf?utm_source=chatgpt.com "NIST.SP.800-53r5.pdf"
[7]: https://www.exida.com/articles/IEC-61508-Overview.pdf?utm_source=chatgpt.com "IEC 61508 Overview Report"
[8]: https://csf.tools/reference/nist-sp-800-53/r5/ir/ir-2/?utm_source=chatgpt.com "IR-2: Incident Response Training"
[9]: https://www.iso.org/standard/70918.html?utm_source=chatgpt.com "ISO/SAE 21434:2021 - Road vehicles — Cybersecurity ..."
